{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransformerとTokenizerは.pyで保存済みなため学習ループの実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoshida/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トランスフォーマーの総パラメータ数: 85.62M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from GPT2 import Transformer, ModelConfig\n",
    "from Tokenizer import Tokenizer\n",
    "\n",
    "# データセットの作成\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.tokens = self.tokenizer.encode(text, eot=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.tokens[idx:idx+self.block_size]), torch.tensor(self.tokens[idx+1:idx+self.block_size+1])\n",
    "\n",
    "# データセットの読み込み\n",
    "# 相対パスを指定してテキストファイルを読み込む\n",
    "file_path1 = \"../data/mini_ptb.train.txt\"\n",
    "file_path2 = \"../data/mini_ptb.valid.txt\"\n",
    "file_path3 = \"../data/mini_ptb.test.txt\"\n",
    "\n",
    "# ファイルを開いて内容を読み込む\n",
    "with open(file_path1, \"r\", encoding=\"utf-8\") as file:\n",
    "    mini_ptb_train_text = file.read()\n",
    "\n",
    "with open(file_path2, \"r\", encoding=\"utf-8\") as file:\n",
    "    mini_ptb_valid_text = file.read()\n",
    "\n",
    "with open(file_path3, \"r\", encoding=\"utf-8\") as file:\n",
    "    mini_ptb_test_text = file.read()\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizerの初期化\n",
    "unique_chars_in_train_text = sorted(list(set(mini_ptb_train_text)))\n",
    "tokenizer = Tokenizer(unique_chars_in_train_text)\n",
    "\n",
    "# モデルの初期化\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "block_size = 128\n",
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "config = ModelConfig(\n",
    "    block_size=128,    # シーケンス長\n",
    "    vocab_size= vocab_size,  # ボキャブラリサイズ（例: GPTのボキャブラリ）\n",
    "    n_layer=12,        # 層数\n",
    "    n_embd=768,        # 埋め込み次元\n",
    "    n_head=12,         # アテンションヘッド\n",
    "    dropout=0.1        # ドロップアウト率\n",
    ")\n",
    "\n",
    "transformer = Transformer(config).to(device)\n",
    "\n",
    "# データセットの作成\n",
    "train_dataset = TextDataset(mini_ptb_train_text, tokenizer, block_size)\n",
    "valid_dataset = TextDataset(mini_ptb_valid_text, tokenizer, block_size)\n",
    "test_dataset = TextDataset(mini_ptb_test_text, tokenizer, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 学習ループの実装\n",
    "def train(model, dataloader, optimizer, epochs, device):\n",
    "    model.train()  # モデルを訓練モードに設定\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            # モデルの順伝播\n",
    "            logits, loss = model(input_ids, targets=target_ids)\n",
    "\n",
    "            # 損失の計算と逆伝播\n",
    "            optimizer.zero_grad()  # 勾配の初期化\n",
    "            loss.backward()  # 逆伝播\n",
    "            optimizer.step()  # パラメータ更新\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if i % 100 == 99:  # 100ステップごとに進捗を表示\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], Loss: {running_loss / 100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print(\"学習完了\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/60207], Loss: 2.4079\n",
      "Epoch [1/1], Step [200/60207], Loss: 2.3676\n",
      "Epoch [1/1], Step [300/60207], Loss: 2.3404\n",
      "Epoch [1/1], Step [400/60207], Loss: 2.2909\n",
      "Epoch [1/1], Step [500/60207], Loss: 2.2473\n",
      "Epoch [1/1], Step [600/60207], Loss: 2.1701\n",
      "Epoch [1/1], Step [700/60207], Loss: 2.1169\n",
      "Epoch [1/1], Step [800/60207], Loss: 2.0700\n",
      "Epoch [1/1], Step [900/60207], Loss: 2.0246\n",
      "Epoch [1/1], Step [1000/60207], Loss: 2.0154\n",
      "Epoch [1/1], Step [1100/60207], Loss: 1.9968\n",
      "Epoch [1/1], Step [1200/60207], Loss: 1.9710\n",
      "Epoch [1/1], Step [1300/60207], Loss: 1.9761\n",
      "Epoch [1/1], Step [1400/60207], Loss: 1.9243\n",
      "Epoch [1/1], Step [1500/60207], Loss: 1.9027\n",
      "Epoch [1/1], Step [1600/60207], Loss: 1.8813\n",
      "Epoch [1/1], Step [1700/60207], Loss: 1.8733\n",
      "Epoch [1/1], Step [1800/60207], Loss: 1.8457\n",
      "Epoch [1/1], Step [1900/60207], Loss: 1.8372\n",
      "Epoch [1/1], Step [2000/60207], Loss: 1.8134\n",
      "Epoch [1/1], Step [2100/60207], Loss: 1.7872\n",
      "Epoch [1/1], Step [2200/60207], Loss: 1.7567\n",
      "Epoch [1/1], Step [2300/60207], Loss: 1.7553\n",
      "Epoch [1/1], Step [2400/60207], Loss: 1.7807\n",
      "Epoch [1/1], Step [2500/60207], Loss: 1.7175\n",
      "Epoch [1/1], Step [2600/60207], Loss: 1.7102\n",
      "Epoch [1/1], Step [2700/60207], Loss: 1.7124\n",
      "Epoch [1/1], Step [2800/60207], Loss: 1.6746\n",
      "Epoch [1/1], Step [2900/60207], Loss: 1.6722\n",
      "Epoch [1/1], Step [3000/60207], Loss: 1.6489\n",
      "Epoch [1/1], Step [3100/60207], Loss: 1.6457\n",
      "Epoch [1/1], Step [3200/60207], Loss: 1.6618\n",
      "Epoch [1/1], Step [3300/60207], Loss: 1.6303\n",
      "Epoch [1/1], Step [3400/60207], Loss: 1.6398\n",
      "Epoch [1/1], Step [3500/60207], Loss: 1.6261\n",
      "Epoch [1/1], Step [3600/60207], Loss: 1.6086\n",
      "Epoch [1/1], Step [3700/60207], Loss: 1.6088\n",
      "Epoch [1/1], Step [3800/60207], Loss: 1.6116\n",
      "Epoch [1/1], Step [3900/60207], Loss: 1.5973\n",
      "Epoch [1/1], Step [4000/60207], Loss: 1.5929\n",
      "Epoch [1/1], Step [4100/60207], Loss: 1.5688\n",
      "Epoch [1/1], Step [4200/60207], Loss: 1.5445\n",
      "Epoch [1/1], Step [4300/60207], Loss: 1.5666\n",
      "Epoch [1/1], Step [4400/60207], Loss: 1.5472\n",
      "Epoch [1/1], Step [4500/60207], Loss: 1.5591\n",
      "Epoch [1/1], Step [4600/60207], Loss: 1.5489\n",
      "Epoch [1/1], Step [4700/60207], Loss: 1.5421\n",
      "Epoch [1/1], Step [4800/60207], Loss: 1.5170\n",
      "Epoch [1/1], Step [4900/60207], Loss: 1.5324\n",
      "Epoch [1/1], Step [5000/60207], Loss: 1.5159\n",
      "Epoch [1/1], Step [5100/60207], Loss: 1.5161\n",
      "Epoch [1/1], Step [5200/60207], Loss: 1.5118\n",
      "Epoch [1/1], Step [5300/60207], Loss: 1.5059\n",
      "Epoch [1/1], Step [5400/60207], Loss: 1.4801\n",
      "Epoch [1/1], Step [5500/60207], Loss: 1.4765\n",
      "Epoch [1/1], Step [5600/60207], Loss: 1.4799\n",
      "Epoch [1/1], Step [5700/60207], Loss: 1.4933\n",
      "Epoch [1/1], Step [5800/60207], Loss: 1.4786\n",
      "Epoch [1/1], Step [5900/60207], Loss: 1.4601\n",
      "Epoch [1/1], Step [6000/60207], Loss: 1.4445\n",
      "Epoch [1/1], Step [6100/60207], Loss: 1.4581\n",
      "Epoch [1/1], Step [6200/60207], Loss: 1.4499\n",
      "Epoch [1/1], Step [6300/60207], Loss: 1.4184\n",
      "Epoch [1/1], Step [6400/60207], Loss: 1.4250\n",
      "Epoch [1/1], Step [6500/60207], Loss: 1.4299\n",
      "Epoch [1/1], Step [6600/60207], Loss: 1.4136\n",
      "Epoch [1/1], Step [6700/60207], Loss: 1.4318\n",
      "Epoch [1/1], Step [6800/60207], Loss: 1.4340\n",
      "Epoch [1/1], Step [6900/60207], Loss: 1.4281\n",
      "Epoch [1/1], Step [7000/60207], Loss: 1.3918\n",
      "Epoch [1/1], Step [7100/60207], Loss: 1.3868\n",
      "Epoch [1/1], Step [7200/60207], Loss: 1.4048\n",
      "Epoch [1/1], Step [7300/60207], Loss: 1.3842\n",
      "Epoch [1/1], Step [7400/60207], Loss: 1.3862\n",
      "Epoch [1/1], Step [7500/60207], Loss: 1.3867\n",
      "Epoch [1/1], Step [7600/60207], Loss: 1.3821\n",
      "Epoch [1/1], Step [7700/60207], Loss: 1.3504\n",
      "Epoch [1/1], Step [7800/60207], Loss: 1.3913\n",
      "Epoch [1/1], Step [7900/60207], Loss: 1.3655\n",
      "Epoch [1/1], Step [8000/60207], Loss: 1.3710\n",
      "Epoch [1/1], Step [8100/60207], Loss: 1.3581\n",
      "Epoch [1/1], Step [8200/60207], Loss: 1.3425\n",
      "Epoch [1/1], Step [8300/60207], Loss: 1.3437\n",
      "Epoch [1/1], Step [8400/60207], Loss: 1.3470\n",
      "Epoch [1/1], Step [8500/60207], Loss: 1.3628\n",
      "Epoch [1/1], Step [8600/60207], Loss: 1.3340\n",
      "Epoch [1/1], Step [8700/60207], Loss: 1.3411\n",
      "Epoch [1/1], Step [8800/60207], Loss: 1.3501\n",
      "Epoch [1/1], Step [8900/60207], Loss: 1.3436\n",
      "Epoch [1/1], Step [9000/60207], Loss: 1.3344\n",
      "Epoch [1/1], Step [9100/60207], Loss: 1.3546\n",
      "Epoch [1/1], Step [9200/60207], Loss: 1.3164\n",
      "Epoch [1/1], Step [9300/60207], Loss: 1.3236\n",
      "Epoch [1/1], Step [9400/60207], Loss: 1.3083\n",
      "Epoch [1/1], Step [9500/60207], Loss: 1.3429\n",
      "Epoch [1/1], Step [9600/60207], Loss: 1.3087\n",
      "Epoch [1/1], Step [9700/60207], Loss: 1.3156\n",
      "Epoch [1/1], Step [9800/60207], Loss: 1.3071\n",
      "Epoch [1/1], Step [9900/60207], Loss: 1.3206\n",
      "Epoch [1/1], Step [10000/60207], Loss: 1.3059\n",
      "Epoch [1/1], Step [10100/60207], Loss: 1.2990\n",
      "Epoch [1/1], Step [10200/60207], Loss: 1.2938\n",
      "Epoch [1/1], Step [10300/60207], Loss: 1.3044\n",
      "Epoch [1/1], Step [10400/60207], Loss: 1.2656\n",
      "Epoch [1/1], Step [10500/60207], Loss: 1.2953\n",
      "Epoch [1/1], Step [10600/60207], Loss: 1.2835\n",
      "Epoch [1/1], Step [10700/60207], Loss: 1.2758\n",
      "Epoch [1/1], Step [10800/60207], Loss: 1.2746\n",
      "Epoch [1/1], Step [10900/60207], Loss: 1.2794\n",
      "Epoch [1/1], Step [11000/60207], Loss: 1.2782\n",
      "Epoch [1/1], Step [11100/60207], Loss: 1.2778\n",
      "Epoch [1/1], Step [11200/60207], Loss: 1.2820\n",
      "Epoch [1/1], Step [11300/60207], Loss: 1.2593\n",
      "Epoch [1/1], Step [11400/60207], Loss: 1.2600\n",
      "Epoch [1/1], Step [11500/60207], Loss: 1.2504\n",
      "Epoch [1/1], Step [11600/60207], Loss: 1.2628\n",
      "Epoch [1/1], Step [11700/60207], Loss: 1.2820\n",
      "Epoch [1/1], Step [11800/60207], Loss: 1.2619\n",
      "Epoch [1/1], Step [11900/60207], Loss: 1.2442\n",
      "Epoch [1/1], Step [12000/60207], Loss: 1.2483\n",
      "Epoch [1/1], Step [12100/60207], Loss: 1.2483\n",
      "Epoch [1/1], Step [12200/60207], Loss: 1.2667\n",
      "Epoch [1/1], Step [12300/60207], Loss: 1.2458\n",
      "Epoch [1/1], Step [12400/60207], Loss: 1.2604\n",
      "Epoch [1/1], Step [12500/60207], Loss: 1.2460\n",
      "Epoch [1/1], Step [12600/60207], Loss: 1.2444\n",
      "Epoch [1/1], Step [12700/60207], Loss: 1.2323\n",
      "Epoch [1/1], Step [12800/60207], Loss: 1.2426\n",
      "Epoch [1/1], Step [12900/60207], Loss: 1.2544\n",
      "Epoch [1/1], Step [13000/60207], Loss: 1.2212\n",
      "Epoch [1/1], Step [13100/60207], Loss: 1.2445\n",
      "Epoch [1/1], Step [13200/60207], Loss: 1.2407\n",
      "Epoch [1/1], Step [13300/60207], Loss: 1.2403\n",
      "Epoch [1/1], Step [13400/60207], Loss: 1.2298\n",
      "Epoch [1/1], Step [13500/60207], Loss: 1.2178\n",
      "Epoch [1/1], Step [13600/60207], Loss: 1.2005\n",
      "Epoch [1/1], Step [13700/60207], Loss: 1.2287\n",
      "Epoch [1/1], Step [13800/60207], Loss: 1.2257\n",
      "Epoch [1/1], Step [13900/60207], Loss: 1.2113\n",
      "Epoch [1/1], Step [14000/60207], Loss: 1.2313\n",
      "Epoch [1/1], Step [14100/60207], Loss: 1.2228\n",
      "Epoch [1/1], Step [14200/60207], Loss: 1.2294\n",
      "Epoch [1/1], Step [14300/60207], Loss: 1.2425\n",
      "Epoch [1/1], Step [14400/60207], Loss: 1.2158\n",
      "Epoch [1/1], Step [14500/60207], Loss: 1.2234\n",
      "Epoch [1/1], Step [14600/60207], Loss: 1.2122\n",
      "Epoch [1/1], Step [14700/60207], Loss: 1.2213\n",
      "Epoch [1/1], Step [14800/60207], Loss: 1.2024\n",
      "Epoch [1/1], Step [14900/60207], Loss: 1.2281\n",
      "Epoch [1/1], Step [15000/60207], Loss: 1.2166\n",
      "Epoch [1/1], Step [15100/60207], Loss: 1.2263\n",
      "Epoch [1/1], Step [15200/60207], Loss: 1.2013\n",
      "Epoch [1/1], Step [15300/60207], Loss: 1.1994\n",
      "Epoch [1/1], Step [15400/60207], Loss: 1.2089\n",
      "Epoch [1/1], Step [15500/60207], Loss: 1.2031\n",
      "Epoch [1/1], Step [15600/60207], Loss: 1.1997\n",
      "Epoch [1/1], Step [15700/60207], Loss: 1.1907\n",
      "Epoch [1/1], Step [15800/60207], Loss: 1.1889\n",
      "Epoch [1/1], Step [15900/60207], Loss: 1.1759\n",
      "Epoch [1/1], Step [16000/60207], Loss: 1.1668\n",
      "Epoch [1/1], Step [16100/60207], Loss: 1.1986\n",
      "Epoch [1/1], Step [16200/60207], Loss: 1.1995\n",
      "Epoch [1/1], Step [16300/60207], Loss: 1.1882\n",
      "Epoch [1/1], Step [16400/60207], Loss: 1.1866\n",
      "Epoch [1/1], Step [16500/60207], Loss: 1.1713\n",
      "Epoch [1/1], Step [16600/60207], Loss: 1.1913\n",
      "Epoch [1/1], Step [16700/60207], Loss: 1.1871\n",
      "Epoch [1/1], Step [16800/60207], Loss: 1.1991\n",
      "Epoch [1/1], Step [16900/60207], Loss: 1.1768\n",
      "Epoch [1/1], Step [17000/60207], Loss: 1.1719\n",
      "Epoch [1/1], Step [17100/60207], Loss: 1.1786\n",
      "Epoch [1/1], Step [17200/60207], Loss: 1.2010\n",
      "Epoch [1/1], Step [17300/60207], Loss: 1.2079\n",
      "Epoch [1/1], Step [17400/60207], Loss: 1.2027\n",
      "Epoch [1/1], Step [17500/60207], Loss: 1.1817\n",
      "Epoch [1/1], Step [17600/60207], Loss: 1.1771\n",
      "Epoch [1/1], Step [17700/60207], Loss: 1.1853\n",
      "Epoch [1/1], Step [17800/60207], Loss: 1.1902\n",
      "Epoch [1/1], Step [17900/60207], Loss: 1.1754\n",
      "Epoch [1/1], Step [18000/60207], Loss: 1.1654\n",
      "Epoch [1/1], Step [18100/60207], Loss: 1.1632\n",
      "Epoch [1/1], Step [18200/60207], Loss: 1.1633\n",
      "Epoch [1/1], Step [18300/60207], Loss: 1.1761\n",
      "Epoch [1/1], Step [18400/60207], Loss: 1.1541\n",
      "Epoch [1/1], Step [18500/60207], Loss: 1.1291\n",
      "Epoch [1/1], Step [18600/60207], Loss: 1.1443\n",
      "Epoch [1/1], Step [18700/60207], Loss: 1.1525\n",
      "Epoch [1/1], Step [18800/60207], Loss: 1.1531\n",
      "Epoch [1/1], Step [18900/60207], Loss: 1.1565\n",
      "Epoch [1/1], Step [19000/60207], Loss: 1.1627\n",
      "Epoch [1/1], Step [19100/60207], Loss: 1.1449\n",
      "Epoch [1/1], Step [19200/60207], Loss: 1.1309\n",
      "Epoch [1/1], Step [19300/60207], Loss: 1.1369\n",
      "Epoch [1/1], Step [19400/60207], Loss: 1.1481\n",
      "Epoch [1/1], Step [19500/60207], Loss: 1.1389\n",
      "Epoch [1/1], Step [19600/60207], Loss: 1.1224\n",
      "Epoch [1/1], Step [19700/60207], Loss: 1.1387\n",
      "Epoch [1/1], Step [19800/60207], Loss: 1.1273\n",
      "Epoch [1/1], Step [19900/60207], Loss: 1.1170\n",
      "Epoch [1/1], Step [20000/60207], Loss: 1.1473\n",
      "Epoch [1/1], Step [20100/60207], Loss: 1.1548\n",
      "Epoch [1/1], Step [20200/60207], Loss: 1.1363\n",
      "Epoch [1/1], Step [20300/60207], Loss: 1.1153\n",
      "Epoch [1/1], Step [20400/60207], Loss: 1.1082\n",
      "Epoch [1/1], Step [20500/60207], Loss: 1.1539\n",
      "Epoch [1/1], Step [20600/60207], Loss: 1.1293\n",
      "Epoch [1/1], Step [20700/60207], Loss: 1.1112\n",
      "Epoch [1/1], Step [20800/60207], Loss: 1.1209\n",
      "Epoch [1/1], Step [20900/60207], Loss: 1.1234\n",
      "Epoch [1/1], Step [21000/60207], Loss: 1.0964\n",
      "Epoch [1/1], Step [21100/60207], Loss: 1.1057\n",
      "Epoch [1/1], Step [21200/60207], Loss: 1.0928\n",
      "Epoch [1/1], Step [21300/60207], Loss: 1.0692\n",
      "Epoch [1/1], Step [21400/60207], Loss: 1.0998\n",
      "Epoch [1/1], Step [21500/60207], Loss: 1.1114\n",
      "Epoch [1/1], Step [21600/60207], Loss: 1.1140\n",
      "Epoch [1/1], Step [21700/60207], Loss: 1.0915\n",
      "Epoch [1/1], Step [21800/60207], Loss: 1.1116\n",
      "Epoch [1/1], Step [21900/60207], Loss: 1.0909\n",
      "Epoch [1/1], Step [22000/60207], Loss: 1.0955\n",
      "Epoch [1/1], Step [22100/60207], Loss: 1.0797\n",
      "Epoch [1/1], Step [22200/60207], Loss: 1.0944\n",
      "Epoch [1/1], Step [22300/60207], Loss: 1.1048\n",
      "Epoch [1/1], Step [22400/60207], Loss: 1.0990\n",
      "Epoch [1/1], Step [22500/60207], Loss: 1.0698\n",
      "Epoch [1/1], Step [22600/60207], Loss: 1.0665\n",
      "Epoch [1/1], Step [22700/60207], Loss: 1.0667\n",
      "Epoch [1/1], Step [22800/60207], Loss: 1.0979\n",
      "Epoch [1/1], Step [22900/60207], Loss: 1.0930\n",
      "Epoch [1/1], Step [23000/60207], Loss: 1.0802\n",
      "Epoch [1/1], Step [23100/60207], Loss: 1.0786\n",
      "Epoch [1/1], Step [23200/60207], Loss: 1.0907\n",
      "Epoch [1/1], Step [23300/60207], Loss: 1.0895\n",
      "Epoch [1/1], Step [23400/60207], Loss: 1.0992\n",
      "Epoch [1/1], Step [23500/60207], Loss: 1.0910\n",
      "Epoch [1/1], Step [23600/60207], Loss: 1.0681\n",
      "Epoch [1/1], Step [23700/60207], Loss: 1.0650\n",
      "Epoch [1/1], Step [23800/60207], Loss: 1.0826\n",
      "Epoch [1/1], Step [23900/60207], Loss: 1.0730\n",
      "Epoch [1/1], Step [24000/60207], Loss: 1.0389\n",
      "Epoch [1/1], Step [24100/60207], Loss: 1.0500\n",
      "Epoch [1/1], Step [24200/60207], Loss: 1.0508\n",
      "Epoch [1/1], Step [24300/60207], Loss: 1.0607\n",
      "Epoch [1/1], Step [24400/60207], Loss: 1.0478\n",
      "Epoch [1/1], Step [24500/60207], Loss: 1.0691\n",
      "Epoch [1/1], Step [24600/60207], Loss: 1.0605\n",
      "Epoch [1/1], Step [24700/60207], Loss: 1.0573\n",
      "Epoch [1/1], Step [24800/60207], Loss: 1.0541\n",
      "Epoch [1/1], Step [24900/60207], Loss: 1.0433\n",
      "Epoch [1/1], Step [25000/60207], Loss: 1.0249\n",
      "Epoch [1/1], Step [25100/60207], Loss: 1.0461\n",
      "Epoch [1/1], Step [25200/60207], Loss: 1.0553\n",
      "Epoch [1/1], Step [25300/60207], Loss: 1.0663\n",
      "Epoch [1/1], Step [25400/60207], Loss: 1.0367\n",
      "Epoch [1/1], Step [25500/60207], Loss: 1.0382\n",
      "Epoch [1/1], Step [25600/60207], Loss: 1.0325\n",
      "Epoch [1/1], Step [25700/60207], Loss: 1.0343\n",
      "Epoch [1/1], Step [25800/60207], Loss: 1.0850\n",
      "Epoch [1/1], Step [25900/60207], Loss: 1.0375\n",
      "Epoch [1/1], Step [26000/60207], Loss: 1.0561\n",
      "Epoch [1/1], Step [26100/60207], Loss: 1.0347\n",
      "Epoch [1/1], Step [26200/60207], Loss: 1.0298\n",
      "Epoch [1/1], Step [26300/60207], Loss: 1.0263\n",
      "Epoch [1/1], Step [26400/60207], Loss: 1.0251\n",
      "Epoch [1/1], Step [26500/60207], Loss: 1.0354\n",
      "Epoch [1/1], Step [26600/60207], Loss: 1.0287\n",
      "Epoch [1/1], Step [26700/60207], Loss: 1.0292\n",
      "Epoch [1/1], Step [26800/60207], Loss: 1.0024\n",
      "Epoch [1/1], Step [26900/60207], Loss: 1.0038\n",
      "Epoch [1/1], Step [27000/60207], Loss: 1.0004\n",
      "Epoch [1/1], Step [27100/60207], Loss: 1.0351\n",
      "Epoch [1/1], Step [27200/60207], Loss: 1.0181\n",
      "Epoch [1/1], Step [27300/60207], Loss: 1.0105\n",
      "Epoch [1/1], Step [27400/60207], Loss: 1.0091\n",
      "Epoch [1/1], Step [27500/60207], Loss: 1.0188\n",
      "Epoch [1/1], Step [27600/60207], Loss: 1.0294\n",
      "Epoch [1/1], Step [27700/60207], Loss: 0.9931\n",
      "Epoch [1/1], Step [27800/60207], Loss: 1.0027\n",
      "Epoch [1/1], Step [27900/60207], Loss: 1.0035\n",
      "Epoch [1/1], Step [28000/60207], Loss: 1.0203\n",
      "Epoch [1/1], Step [28100/60207], Loss: 1.0789\n",
      "Epoch [1/1], Step [28200/60207], Loss: 1.0250\n",
      "Epoch [1/1], Step [28300/60207], Loss: 0.9785\n",
      "Epoch [1/1], Step [28400/60207], Loss: 0.9824\n",
      "Epoch [1/1], Step [28500/60207], Loss: 1.0061\n",
      "Epoch [1/1], Step [28600/60207], Loss: 0.9979\n",
      "Epoch [1/1], Step [28700/60207], Loss: 0.9716\n",
      "Epoch [1/1], Step [28800/60207], Loss: 1.0002\n",
      "Epoch [1/1], Step [28900/60207], Loss: 0.9920\n",
      "Epoch [1/1], Step [29000/60207], Loss: 0.9895\n",
      "Epoch [1/1], Step [29100/60207], Loss: 0.9673\n",
      "Epoch [1/1], Step [29200/60207], Loss: 0.9666\n",
      "Epoch [1/1], Step [29300/60207], Loss: 0.9679\n",
      "Epoch [1/1], Step [29400/60207], Loss: 0.9824\n",
      "Epoch [1/1], Step [29500/60207], Loss: 0.9672\n",
      "Epoch [1/1], Step [29600/60207], Loss: 0.9393\n",
      "Epoch [1/1], Step [29700/60207], Loss: 0.9394\n",
      "Epoch [1/1], Step [29800/60207], Loss: 0.9481\n",
      "Epoch [1/1], Step [29900/60207], Loss: 0.9146\n",
      "Epoch [1/1], Step [30000/60207], Loss: 0.9969\n",
      "Epoch [1/1], Step [30100/60207], Loss: 0.9936\n",
      "Epoch [1/1], Step [30200/60207], Loss: 0.9718\n",
      "Epoch [1/1], Step [30300/60207], Loss: 0.9469\n",
      "Epoch [1/1], Step [30400/60207], Loss: 0.9526\n",
      "Epoch [1/1], Step [30500/60207], Loss: 0.9306\n",
      "Epoch [1/1], Step [30600/60207], Loss: 0.9547\n",
      "Epoch [1/1], Step [30700/60207], Loss: 0.9295\n",
      "Epoch [1/1], Step [30800/60207], Loss: 0.9548\n",
      "Epoch [1/1], Step [30900/60207], Loss: 0.9407\n",
      "Epoch [1/1], Step [31000/60207], Loss: 0.9258\n",
      "Epoch [1/1], Step [31100/60207], Loss: 0.9538\n",
      "Epoch [1/1], Step [31200/60207], Loss: 0.9419\n",
      "Epoch [1/1], Step [31300/60207], Loss: 0.9285\n",
      "Epoch [1/1], Step [31400/60207], Loss: 0.9569\n",
      "Epoch [1/1], Step [31500/60207], Loss: 0.9924\n",
      "Epoch [1/1], Step [31600/60207], Loss: 0.9446\n",
      "Epoch [1/1], Step [31700/60207], Loss: 0.9050\n",
      "Epoch [1/1], Step [31800/60207], Loss: 0.8910\n",
      "Epoch [1/1], Step [31900/60207], Loss: 0.9092\n",
      "Epoch [1/1], Step [32000/60207], Loss: 0.8842\n",
      "Epoch [1/1], Step [32100/60207], Loss: 0.9107\n",
      "Epoch [1/1], Step [32200/60207], Loss: 0.9191\n",
      "Epoch [1/1], Step [32300/60207], Loss: 0.9277\n",
      "Epoch [1/1], Step [32400/60207], Loss: 0.8951\n",
      "Epoch [1/1], Step [32500/60207], Loss: 0.8825\n",
      "Epoch [1/1], Step [32600/60207], Loss: 0.9023\n",
      "Epoch [1/1], Step [32700/60207], Loss: 0.9109\n",
      "Epoch [1/1], Step [32800/60207], Loss: 0.8927\n",
      "Epoch [1/1], Step [32900/60207], Loss: 0.8780\n",
      "Epoch [1/1], Step [33000/60207], Loss: 0.8652\n",
      "Epoch [1/1], Step [33100/60207], Loss: 0.8373\n",
      "Epoch [1/1], Step [33200/60207], Loss: 0.8789\n",
      "Epoch [1/1], Step [33300/60207], Loss: 0.8688\n",
      "Epoch [1/1], Step [33400/60207], Loss: 0.8656\n",
      "Epoch [1/1], Step [33500/60207], Loss: 0.8655\n",
      "Epoch [1/1], Step [33600/60207], Loss: 0.8824\n",
      "Epoch [1/1], Step [33700/60207], Loss: 0.8643\n",
      "Epoch [1/1], Step [33800/60207], Loss: 0.8316\n",
      "Epoch [1/1], Step [33900/60207], Loss: 0.8248\n",
      "Epoch [1/1], Step [34000/60207], Loss: 0.8268\n",
      "Epoch [1/1], Step [34100/60207], Loss: 0.8253\n",
      "Epoch [1/1], Step [34200/60207], Loss: 0.8522\n",
      "Epoch [1/1], Step [34300/60207], Loss: 0.8402\n",
      "Epoch [1/1], Step [34400/60207], Loss: 0.8608\n",
      "Epoch [1/1], Step [34500/60207], Loss: 0.8221\n",
      "Epoch [1/1], Step [34600/60207], Loss: 0.8161\n",
      "Epoch [1/1], Step [34700/60207], Loss: 0.8109\n",
      "Epoch [1/1], Step [34800/60207], Loss: 0.7783\n",
      "Epoch [1/1], Step [34900/60207], Loss: 0.7833\n",
      "Epoch [1/1], Step [35000/60207], Loss: 0.7609\n",
      "Epoch [1/1], Step [35100/60207], Loss: 0.7900\n",
      "Epoch [1/1], Step [35200/60207], Loss: 0.7950\n",
      "Epoch [1/1], Step [35300/60207], Loss: 0.7758\n",
      "Epoch [1/1], Step [35400/60207], Loss: 0.7931\n",
      "Epoch [1/1], Step [35500/60207], Loss: 0.7550\n",
      "Epoch [1/1], Step [35600/60207], Loss: 0.7819\n",
      "Epoch [1/1], Step [35700/60207], Loss: 0.7853\n",
      "Epoch [1/1], Step [35800/60207], Loss: 0.7730\n",
      "Epoch [1/1], Step [35900/60207], Loss: 0.7702\n",
      "Epoch [1/1], Step [36000/60207], Loss: 0.7465\n",
      "Epoch [1/1], Step [36100/60207], Loss: 0.7755\n",
      "Epoch [1/1], Step [36200/60207], Loss: 0.7662\n",
      "Epoch [1/1], Step [36300/60207], Loss: 0.7423\n",
      "Epoch [1/1], Step [36400/60207], Loss: 0.7270\n",
      "Epoch [1/1], Step [36500/60207], Loss: 0.7548\n",
      "Epoch [1/1], Step [36600/60207], Loss: 0.7333\n",
      "Epoch [1/1], Step [36700/60207], Loss: 0.7230\n",
      "Epoch [1/1], Step [36800/60207], Loss: 0.6925\n",
      "Epoch [1/1], Step [36900/60207], Loss: 0.7122\n",
      "Epoch [1/1], Step [37000/60207], Loss: 0.7225\n",
      "Epoch [1/1], Step [37100/60207], Loss: 0.7246\n",
      "Epoch [1/1], Step [37200/60207], Loss: 0.7223\n",
      "Epoch [1/1], Step [37300/60207], Loss: 0.7151\n",
      "Epoch [1/1], Step [37400/60207], Loss: 0.7439\n",
      "Epoch [1/1], Step [37500/60207], Loss: 0.7216\n",
      "Epoch [1/1], Step [37600/60207], Loss: 0.6987\n",
      "Epoch [1/1], Step [37700/60207], Loss: 0.7119\n",
      "Epoch [1/1], Step [37800/60207], Loss: 0.6911\n",
      "Epoch [1/1], Step [37900/60207], Loss: 0.7039\n",
      "Epoch [1/1], Step [38000/60207], Loss: 0.6769\n",
      "Epoch [1/1], Step [38100/60207], Loss: 0.7012\n",
      "Epoch [1/1], Step [38200/60207], Loss: 0.6681\n",
      "Epoch [1/1], Step [38300/60207], Loss: 0.6659\n",
      "Epoch [1/1], Step [38400/60207], Loss: 0.6782\n",
      "Epoch [1/1], Step [38500/60207], Loss: 0.6667\n",
      "Epoch [1/1], Step [38600/60207], Loss: 0.6468\n",
      "Epoch [1/1], Step [38700/60207], Loss: 0.6557\n",
      "Epoch [1/1], Step [38800/60207], Loss: 0.6367\n",
      "Epoch [1/1], Step [38900/60207], Loss: 0.6361\n",
      "Epoch [1/1], Step [39000/60207], Loss: 0.6592\n",
      "Epoch [1/1], Step [39100/60207], Loss: 0.6725\n",
      "Epoch [1/1], Step [39200/60207], Loss: 0.6589\n",
      "Epoch [1/1], Step [39300/60207], Loss: 0.6270\n",
      "Epoch [1/1], Step [39400/60207], Loss: 0.6125\n",
      "Epoch [1/1], Step [39500/60207], Loss: 0.6233\n",
      "Epoch [1/1], Step [39600/60207], Loss: 0.6252\n",
      "Epoch [1/1], Step [39700/60207], Loss: 0.6121\n",
      "Epoch [1/1], Step [39800/60207], Loss: 0.5958\n",
      "Epoch [1/1], Step [39900/60207], Loss: 0.5695\n",
      "Epoch [1/1], Step [40000/60207], Loss: 0.5963\n",
      "Epoch [1/1], Step [40100/60207], Loss: 0.5993\n",
      "Epoch [1/1], Step [40200/60207], Loss: 0.6078\n",
      "Epoch [1/1], Step [40300/60207], Loss: 0.5969\n",
      "Epoch [1/1], Step [40400/60207], Loss: 0.6006\n",
      "Epoch [1/1], Step [40500/60207], Loss: 0.5952\n",
      "Epoch [1/1], Step [40600/60207], Loss: 0.5836\n",
      "Epoch [1/1], Step [40700/60207], Loss: 0.5837\n",
      "Epoch [1/1], Step [40800/60207], Loss: 0.5849\n",
      "Epoch [1/1], Step [40900/60207], Loss: 0.5569\n",
      "Epoch [1/1], Step [41000/60207], Loss: 0.5702\n",
      "Epoch [1/1], Step [41100/60207], Loss: 0.5663\n",
      "Epoch [1/1], Step [41200/60207], Loss: 0.5570\n",
      "Epoch [1/1], Step [41300/60207], Loss: 0.5503\n",
      "Epoch [1/1], Step [41400/60207], Loss: 0.5461\n",
      "Epoch [1/1], Step [41500/60207], Loss: 0.5242\n",
      "Epoch [1/1], Step [41600/60207], Loss: 0.5552\n",
      "Epoch [1/1], Step [41700/60207], Loss: 0.5606\n",
      "Epoch [1/1], Step [41800/60207], Loss: 0.5721\n",
      "Epoch [1/1], Step [41900/60207], Loss: 0.5590\n",
      "Epoch [1/1], Step [42000/60207], Loss: 0.5515\n",
      "Epoch [1/1], Step [42100/60207], Loss: 0.5417\n",
      "Epoch [1/1], Step [42200/60207], Loss: 0.5391\n",
      "Epoch [1/1], Step [42300/60207], Loss: 0.5179\n",
      "Epoch [1/1], Step [42400/60207], Loss: 0.4966\n",
      "Epoch [1/1], Step [42500/60207], Loss: 0.5055\n",
      "Epoch [1/1], Step [42600/60207], Loss: 0.5322\n",
      "Epoch [1/1], Step [42700/60207], Loss: 0.4869\n",
      "Epoch [1/1], Step [42800/60207], Loss: 0.5126\n",
      "Epoch [1/1], Step [42900/60207], Loss: 0.4939\n",
      "Epoch [1/1], Step [43000/60207], Loss: 0.5082\n",
      "Epoch [1/1], Step [43100/60207], Loss: 0.5299\n",
      "Epoch [1/1], Step [43200/60207], Loss: 0.5142\n",
      "Epoch [1/1], Step [43300/60207], Loss: 0.5029\n",
      "Epoch [1/1], Step [43400/60207], Loss: 0.4760\n",
      "Epoch [1/1], Step [43500/60207], Loss: 0.5044\n",
      "Epoch [1/1], Step [43600/60207], Loss: 0.4875\n",
      "Epoch [1/1], Step [43700/60207], Loss: 0.4657\n",
      "Epoch [1/1], Step [43800/60207], Loss: 0.4589\n",
      "Epoch [1/1], Step [43900/60207], Loss: 0.4867\n",
      "Epoch [1/1], Step [44000/60207], Loss: 0.4468\n",
      "Epoch [1/1], Step [44100/60207], Loss: 0.4620\n",
      "Epoch [1/1], Step [44200/60207], Loss: 0.4798\n",
      "Epoch [1/1], Step [44300/60207], Loss: 0.4598\n",
      "Epoch [1/1], Step [44400/60207], Loss: 0.4502\n",
      "Epoch [1/1], Step [44500/60207], Loss: 0.4509\n",
      "Epoch [1/1], Step [44600/60207], Loss: 0.4525\n",
      "Epoch [1/1], Step [44700/60207], Loss: 0.4636\n",
      "Epoch [1/1], Step [44800/60207], Loss: 0.4496\n",
      "Epoch [1/1], Step [44900/60207], Loss: 0.4672\n",
      "Epoch [1/1], Step [45000/60207], Loss: 0.4435\n",
      "Epoch [1/1], Step [45100/60207], Loss: 0.4595\n",
      "Epoch [1/1], Step [45200/60207], Loss: 0.4253\n",
      "Epoch [1/1], Step [45300/60207], Loss: 0.4356\n",
      "Epoch [1/1], Step [45400/60207], Loss: 0.4579\n",
      "Epoch [1/1], Step [45500/60207], Loss: 0.4482\n",
      "Epoch [1/1], Step [45600/60207], Loss: 0.4304\n",
      "Epoch [1/1], Step [45700/60207], Loss: 0.4448\n",
      "Epoch [1/1], Step [45800/60207], Loss: 0.4331\n",
      "Epoch [1/1], Step [45900/60207], Loss: 0.4268\n",
      "Epoch [1/1], Step [46000/60207], Loss: 0.4143\n",
      "Epoch [1/1], Step [46100/60207], Loss: 0.4201\n",
      "Epoch [1/1], Step [46200/60207], Loss: 0.4084\n",
      "Epoch [1/1], Step [46300/60207], Loss: 0.4169\n",
      "Epoch [1/1], Step [46400/60207], Loss: 0.4237\n",
      "Epoch [1/1], Step [46500/60207], Loss: 0.4131\n",
      "Epoch [1/1], Step [46600/60207], Loss: 0.4063\n",
      "Epoch [1/1], Step [46700/60207], Loss: 0.4150\n",
      "Epoch [1/1], Step [46800/60207], Loss: 0.3970\n",
      "Epoch [1/1], Step [46900/60207], Loss: 0.3993\n",
      "Epoch [1/1], Step [47000/60207], Loss: 0.3959\n",
      "Epoch [1/1], Step [47100/60207], Loss: 0.3972\n",
      "Epoch [1/1], Step [47200/60207], Loss: 0.3841\n",
      "Epoch [1/1], Step [47300/60207], Loss: 0.3952\n",
      "Epoch [1/1], Step [47400/60207], Loss: 0.3861\n",
      "Epoch [1/1], Step [47500/60207], Loss: 0.3739\n",
      "Epoch [1/1], Step [47600/60207], Loss: 0.3931\n",
      "Epoch [1/1], Step [47700/60207], Loss: 0.3959\n",
      "Epoch [1/1], Step [47800/60207], Loss: 0.3753\n",
      "Epoch [1/1], Step [47900/60207], Loss: 0.4140\n",
      "Epoch [1/1], Step [48000/60207], Loss: 0.3923\n",
      "Epoch [1/1], Step [48100/60207], Loss: 0.3989\n",
      "Epoch [1/1], Step [48200/60207], Loss: 0.3753\n",
      "Epoch [1/1], Step [48300/60207], Loss: 0.3709\n",
      "Epoch [1/1], Step [48400/60207], Loss: 0.3577\n",
      "Epoch [1/1], Step [48500/60207], Loss: 0.3709\n",
      "Epoch [1/1], Step [48600/60207], Loss: 0.3659\n",
      "Epoch [1/1], Step [48700/60207], Loss: 0.3631\n",
      "Epoch [1/1], Step [48800/60207], Loss: 0.3678\n",
      "Epoch [1/1], Step [48900/60207], Loss: 0.3642\n",
      "Epoch [1/1], Step [49000/60207], Loss: 0.3572\n",
      "Epoch [1/1], Step [49100/60207], Loss: 0.3725\n",
      "Epoch [1/1], Step [49200/60207], Loss: 0.3636\n",
      "Epoch [1/1], Step [49300/60207], Loss: 0.3519\n",
      "Epoch [1/1], Step [49400/60207], Loss: 0.3451\n",
      "Epoch [1/1], Step [49500/60207], Loss: 0.3471\n",
      "Epoch [1/1], Step [49600/60207], Loss: 0.3452\n",
      "Epoch [1/1], Step [49700/60207], Loss: 0.3487\n",
      "Epoch [1/1], Step [49800/60207], Loss: 0.3556\n",
      "Epoch [1/1], Step [49900/60207], Loss: 0.3530\n",
      "Epoch [1/1], Step [50000/60207], Loss: 0.3544\n",
      "Epoch [1/1], Step [50100/60207], Loss: 0.3589\n",
      "Epoch [1/1], Step [50200/60207], Loss: 0.3460\n",
      "Epoch [1/1], Step [50300/60207], Loss: 0.3346\n",
      "Epoch [1/1], Step [50400/60207], Loss: 0.3510\n",
      "Epoch [1/1], Step [50500/60207], Loss: 0.3348\n",
      "Epoch [1/1], Step [50600/60207], Loss: 0.3331\n",
      "Epoch [1/1], Step [50700/60207], Loss: 0.3392\n",
      "Epoch [1/1], Step [50800/60207], Loss: 0.3421\n",
      "Epoch [1/1], Step [50900/60207], Loss: 0.3352\n",
      "Epoch [1/1], Step [51000/60207], Loss: 0.3326\n",
      "Epoch [1/1], Step [51100/60207], Loss: 0.3257\n",
      "Epoch [1/1], Step [51200/60207], Loss: 0.3227\n",
      "Epoch [1/1], Step [51300/60207], Loss: 0.3303\n",
      "Epoch [1/1], Step [51400/60207], Loss: 0.3406\n",
      "Epoch [1/1], Step [51500/60207], Loss: 0.3346\n",
      "Epoch [1/1], Step [51600/60207], Loss: 0.3330\n",
      "Epoch [1/1], Step [51700/60207], Loss: 0.3167\n",
      "Epoch [1/1], Step [51800/60207], Loss: 0.3161\n",
      "Epoch [1/1], Step [51900/60207], Loss: 0.3097\n",
      "Epoch [1/1], Step [52000/60207], Loss: 0.3140\n",
      "Epoch [1/1], Step [52100/60207], Loss: 0.3595\n",
      "Epoch [1/1], Step [52200/60207], Loss: 0.3273\n",
      "Epoch [1/1], Step [52300/60207], Loss: 0.3082\n",
      "Epoch [1/1], Step [52400/60207], Loss: 0.3038\n",
      "Epoch [1/1], Step [52500/60207], Loss: 0.3007\n",
      "Epoch [1/1], Step [52600/60207], Loss: 0.2977\n",
      "Epoch [1/1], Step [52700/60207], Loss: 0.3045\n",
      "Epoch [1/1], Step [52800/60207], Loss: 0.3131\n",
      "Epoch [1/1], Step [52900/60207], Loss: 0.2859\n",
      "Epoch [1/1], Step [53000/60207], Loss: 0.2995\n",
      "Epoch [1/1], Step [53100/60207], Loss: 0.2998\n",
      "Epoch [1/1], Step [53200/60207], Loss: 0.2936\n",
      "Epoch [1/1], Step [53300/60207], Loss: 0.2970\n",
      "Epoch [1/1], Step [53400/60207], Loss: 0.2889\n",
      "Epoch [1/1], Step [53500/60207], Loss: 0.2986\n",
      "Epoch [1/1], Step [53600/60207], Loss: 0.2917\n",
      "Epoch [1/1], Step [53700/60207], Loss: 0.3022\n",
      "Epoch [1/1], Step [53800/60207], Loss: 0.3010\n",
      "Epoch [1/1], Step [53900/60207], Loss: 0.2889\n",
      "Epoch [1/1], Step [54000/60207], Loss: 0.2979\n",
      "Epoch [1/1], Step [54100/60207], Loss: 0.2863\n",
      "Epoch [1/1], Step [54200/60207], Loss: 0.2871\n",
      "Epoch [1/1], Step [54300/60207], Loss: 0.2864\n",
      "Epoch [1/1], Step [54400/60207], Loss: 0.2920\n",
      "Epoch [1/1], Step [54500/60207], Loss: 0.2914\n",
      "Epoch [1/1], Step [54600/60207], Loss: 0.2809\n",
      "Epoch [1/1], Step [54700/60207], Loss: 0.2779\n",
      "Epoch [1/1], Step [54800/60207], Loss: 0.2793\n",
      "Epoch [1/1], Step [54900/60207], Loss: 0.2809\n",
      "Epoch [1/1], Step [55000/60207], Loss: 0.2752\n",
      "Epoch [1/1], Step [55100/60207], Loss: 0.2898\n",
      "Epoch [1/1], Step [55200/60207], Loss: 0.2823\n",
      "Epoch [1/1], Step [55300/60207], Loss: 0.2747\n",
      "Epoch [1/1], Step [55400/60207], Loss: 0.2822\n",
      "Epoch [1/1], Step [55500/60207], Loss: 0.2788\n",
      "Epoch [1/1], Step [55600/60207], Loss: 0.2799\n",
      "Epoch [1/1], Step [55700/60207], Loss: 0.2803\n",
      "Epoch [1/1], Step [55800/60207], Loss: 0.2754\n",
      "Epoch [1/1], Step [55900/60207], Loss: 0.2755\n",
      "Epoch [1/1], Step [56000/60207], Loss: 0.2774\n",
      "Epoch [1/1], Step [56100/60207], Loss: 0.2841\n",
      "Epoch [1/1], Step [56200/60207], Loss: 0.2829\n",
      "Epoch [1/1], Step [56300/60207], Loss: 0.2634\n",
      "Epoch [1/1], Step [56400/60207], Loss: 0.2650\n",
      "Epoch [1/1], Step [56500/60207], Loss: 0.2700\n",
      "Epoch [1/1], Step [56600/60207], Loss: 0.2603\n",
      "Epoch [1/1], Step [56700/60207], Loss: 0.2624\n",
      "Epoch [1/1], Step [56800/60207], Loss: 0.2584\n",
      "Epoch [1/1], Step [56900/60207], Loss: 0.2649\n",
      "Epoch [1/1], Step [57000/60207], Loss: 0.2549\n",
      "Epoch [1/1], Step [57100/60207], Loss: 0.2613\n",
      "Epoch [1/1], Step [57200/60207], Loss: 0.2599\n",
      "Epoch [1/1], Step [57300/60207], Loss: 0.2704\n",
      "Epoch [1/1], Step [57400/60207], Loss: 0.2593\n",
      "Epoch [1/1], Step [57500/60207], Loss: 0.2578\n",
      "Epoch [1/1], Step [57600/60207], Loss: 0.2644\n",
      "Epoch [1/1], Step [57700/60207], Loss: 0.2560\n",
      "Epoch [1/1], Step [57800/60207], Loss: 0.2536\n",
      "Epoch [1/1], Step [57900/60207], Loss: 0.2627\n",
      "Epoch [1/1], Step [58000/60207], Loss: 0.2616\n",
      "Epoch [1/1], Step [58100/60207], Loss: 0.2571\n",
      "Epoch [1/1], Step [58200/60207], Loss: 0.2500\n",
      "Epoch [1/1], Step [58300/60207], Loss: 0.2493\n",
      "Epoch [1/1], Step [58400/60207], Loss: 0.2510\n",
      "Epoch [1/1], Step [58500/60207], Loss: 0.2501\n",
      "Epoch [1/1], Step [58600/60207], Loss: 0.2558\n",
      "Epoch [1/1], Step [58700/60207], Loss: 0.2514\n",
      "Epoch [1/1], Step [58800/60207], Loss: 0.2479\n",
      "Epoch [1/1], Step [58900/60207], Loss: 0.2460\n",
      "Epoch [1/1], Step [59000/60207], Loss: 0.2585\n",
      "Epoch [1/1], Step [59100/60207], Loss: 0.2531\n",
      "Epoch [1/1], Step [59200/60207], Loss: 0.2476\n",
      "Epoch [1/1], Step [59300/60207], Loss: 0.2564\n",
      "Epoch [1/1], Step [59400/60207], Loss: 0.2428\n",
      "Epoch [1/1], Step [59500/60207], Loss: 0.2370\n",
      "Epoch [1/1], Step [59600/60207], Loss: 0.2394\n",
      "Epoch [1/1], Step [59700/60207], Loss: 0.2360\n",
      "Epoch [1/1], Step [59800/60207], Loss: 0.2459\n",
      "Epoch [1/1], Step [59900/60207], Loss: 0.2424\n",
      "Epoch [1/1], Step [60000/60207], Loss: 0.2419\n",
      "Epoch [1/1], Step [60100/60207], Loss: 0.2430\n",
      "Epoch [1/1], Step [60200/60207], Loss: 0.2395\n",
      "学習完了\n",
      "Validation Loss: 3.0521\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=3e-4)\n",
    "train(transformer, train_loader, optimizer, epochs=1, device=device)\n",
    "\n",
    "# モデルの評価\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()  # モデルを評価モードに設定\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in dataloader:\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            # モデルの順伝播\n",
    "            logits, loss = model(input_ids, targets=target_ids)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# モデルの評価\n",
    "val_loss = evaluate(transformer, valid_loader, device)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.0.weight', 'transformer.h.0.mlp.0.bias', 'transformer.h.0.mlp.3.weight', 'transformer.h.0.mlp.3.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.0.weight', 'transformer.h.1.mlp.0.bias', 'transformer.h.1.mlp.3.weight', 'transformer.h.1.mlp.3.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.0.weight', 'transformer.h.2.mlp.0.bias', 'transformer.h.2.mlp.3.weight', 'transformer.h.2.mlp.3.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.0.weight', 'transformer.h.3.mlp.0.bias', 'transformer.h.3.mlp.3.weight', 'transformer.h.3.mlp.3.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.0.weight', 'transformer.h.4.mlp.0.bias', 'transformer.h.4.mlp.3.weight', 'transformer.h.4.mlp.3.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.0.weight', 'transformer.h.5.mlp.0.bias', 'transformer.h.5.mlp.3.weight', 'transformer.h.5.mlp.3.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.0.weight', 'transformer.h.6.mlp.0.bias', 'transformer.h.6.mlp.3.weight', 'transformer.h.6.mlp.3.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.0.weight', 'transformer.h.7.mlp.0.bias', 'transformer.h.7.mlp.3.weight', 'transformer.h.7.mlp.3.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.0.weight', 'transformer.h.8.mlp.0.bias', 'transformer.h.8.mlp.3.weight', 'transformer.h.8.mlp.3.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.0.weight', 'transformer.h.9.mlp.0.bias', 'transformer.h.9.mlp.3.weight', 'transformer.h.9.mlp.3.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.0.weight', 'transformer.h.10.mlp.0.bias', 'transformer.h.10.mlp.3.weight', 'transformer.h.10.mlp.3.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.0.weight', 'transformer.h.11.mlp.0.bias', 'transformer.h.11.mlp.3.weight', 'transformer.h.11.mlp.3.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])\n"
     ]
    }
   ],
   "source": [
    "save_data = {\n",
    "    \"model_state_dict\": transformer.state_dict(),\n",
    "    \"config\": config\n",
    "}\n",
    "\n",
    "torch.save(save_data, \"transformer_with_config.pth\")\n",
    "\n",
    "print(transformer.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoshida/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/tmp/ipykernel_283155/3813546757.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_data = torch.load(\"transformer_with_config.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トランスフォーマーの総パラメータ数: 85.62M\n"
     ]
    }
   ],
   "source": [
    "# モデルの読み込み\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from GPT2 import Transformer, ModelConfig\n",
    "from Tokenizer import Tokenizer\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.tokens = self.tokenizer.encode(text, eot=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.tokens[idx:idx+self.block_size]), torch.tensor(self.tokens[idx+1:idx+self.block_size+1])\n",
    "\n",
    "# 相対パスを指定してテキストファイルを読み込む\n",
    "file_path1 = \"../data/mini_ptb.train.txt\"\n",
    "file_path2 = \"../data/mini_ptb.valid.txt\"\n",
    "file_path3 = \"../data/mini_ptb.test.txt\"\n",
    "\n",
    "# ファイルを開いて内容を読み込む\n",
    "with open(file_path1, \"r\", encoding=\"utf-8\") as file:\n",
    "    mini_ptb_train_text = file.read()\n",
    "\n",
    "with open(file_path2, \"r\", encoding=\"utf-8\") as file:\n",
    "    mini_ptb_valid_text = file.read()\n",
    "\n",
    "with open(file_path3, \"r\", encoding=\"utf-8\") as file:\n",
    "    mini_ptb_test_text = file.read()\n",
    "\n",
    "unique_chars_in_train_text = sorted(list(set(mini_ptb_train_text)))\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "load_data = torch.load(\"transformer_with_config.pth\")\n",
    "config = load_data[\"config\"]\n",
    "transformer = Transformer(config)\n",
    "transformer.load_state_dict(load_data[\"model_state_dict\"])\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "tokenizer = Tokenizer(unique_chars_in_train_text)\n",
    "# データセットの作成\n",
    "train_dataset = TextDataset(mini_ptb_train_text, tokenizer, block_size)\n",
    "valid_dataset = TextDataset(mini_ptb_valid_text, tokenizer, block_size)\n",
    "test_dataset = TextDataset(mini_ptb_test_text, tokenizer, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'T' 'h' 'e'\n"
     ]
    }
   ],
   "source": [
    "# 必要なトークンをリストに収集\n",
    "context = 'Th'\n",
    "context_token_indexes = torch.tensor(tokenizer.encode(context)).unsqueeze(0).to(device)\n",
    "generated_tokens = transformer.generate(context_token_indexes, max_new_tokens=1, temperature=0.2, top_k=40)\n",
    "generated_text = []\n",
    "\n",
    "for token in generated_tokens[0]:\n",
    "    generated_text.append(tokenizer.decode([token.item()]))\n",
    "\n",
    "# 横向きに表示\n",
    "print(' '.join(repr(token) for token in generated_text))\n",
    "\n",
    "#縦向きに表示\n",
    "#for token in generated_tokens[0]:\n",
    "#    print(repr(tokenizer.decode([token.item()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perplexityの計算\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in dataloader:\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "            _, loss = model(input_ids, targets=target_ids)\n",
    "            total_loss += loss.item()* input_ids.size(1)\n",
    "            total_tokens += input_ids.size(1)\n",
    "\n",
    "    perplexity = math.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.22\n"
     ]
    }
   ],
   "source": [
    "# 例: Perplexityの計算\n",
    "perplexity = calculate_perplexity(transformer, train_loader, device)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文法的誤り訂正タスク\n",
    "def correct_sentence(model, tokenizer, sentence, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    文法的誤りを含む文を訂正し、正しい文を生成する関数。\n",
    "\n",
    "    Args:\n",
    "        model: 学習済みの言語モデル（Transformerモデル）。\n",
    "        tokenizer: トークナイザー（入力文をトークン化し、出力文をデコード）。\n",
    "        sentence (str): 文法的誤りを含む入力文。\n",
    "        max_new_tokens (int): 生成する最大トークン数。\n",
    "\n",
    "    Returns:\n",
    "        corrected_sentence (str): 訂正された文法的に正しい文。\n",
    "    \"\"\"\n",
    "    model.eval()  # モデルを推論モードに設定\n",
    "    # 入力文をトークン化し、テンソルに変換\n",
    "    token_ids = torch.tensor(tokenizer.encode(sentence, eot=True), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # モデルによるテキスト生成\n",
    "    with torch.no_grad():  # 勾配計算を無効化し、推論を高速化\n",
    "        output_ids = model.generate(token_ids, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # 生成されたトークンをデコードして文字列に戻す\n",
    "    corrected_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力文: I am go to school\n",
      "訂正後の文: <unk_74><unk_258><unk_275><unk_287><unk_258><unk_281><unk_289><unk_258><unk_294><unk_289><unk_258><unk_293><unk_277><unk_282><unk_289><unk_289><unk_286><unk_0><unk_258><unk_275><unk_288><unk_278><unk_258><unk_294><unk_279><unk_286><unk_279><unk_277><unk_294><unk_283><unk_289><unk_288><unk_258><unk_289><unk_280><unk_258><unk_294><unk_282><unk_279><unk_258><unk_293><unk_294><unk_295><unk_278><unk_279><unk_288><unk_294><unk_293><unk_258><unk_293><unk_282><unk_279><unk_279><unk_294><unk_293><unk_258><unk_275><unk_288><unk_278><unk_258><unk_281><unk_275><unk_296><unk_279><unk_258><unk_294><unk_282><unk_279>\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"I am go to school\"\n",
    "\n",
    "corrected_sentence = correct_sentence(transformer, tokenizer, input_sentence)\n",
    "print(f\"入力文: {input_sentence}\")\n",
    "print(f\"訂正後の文: {corrected_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 301\n"
     ]
    }
   ],
   "source": [
    "# 生成されたトークンIDのリストを取得\n",
    "generated_ids = tokenizer.encode(corrected_sentence, eot=True)\n",
    "\n",
    "# トークンIDの範囲確認\n",
    "vocab_size = len(tokenizer.idx_to_str)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "for token_id in generated_ids:\n",
    "    if token_id >= vocab_size:\n",
    "        print(f\"Out-of-range token ID detected: {token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 例: クローズテスト\u001b[39;00m\n\u001b[1;32m     17\u001b[0m masked_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe cat sat on the <mask>.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m completed_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mcloze_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_sentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMasked:\u001b[39m\u001b[38;5;124m\"\u001b[39m, masked_sentence)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted:\u001b[39m\u001b[38;5;124m\"\u001b[39m, completed_sentence)\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mcloze_test\u001b[0;34m(model, tokenizer, masked_sentence, mask_token, max_length)\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(masked_sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# マスクされたトークンまでの入力\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, max_new_tokens\u001b[38;5;241m=\u001b[39mmax_length)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "# クローズテスト\n",
    "def cloze_test(model, tokenizer, masked_sentence, mask_token=\"<mask>\", max_length=50):\n",
    "    \"\"\"\n",
    "    マスクされた部分を予測するタスク。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(masked_sentence, return_tensors=\"pt\").to(device)\n",
    "    input_ids = input_ids[:, :-1]  # マスクされたトークンまでの入力\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_new_tokens=max_length)\n",
    "\n",
    "    completed_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return completed_sentence\n",
    "\n",
    "# 例: クローズテスト\n",
    "masked_sentence = \"The cat sat on the <mask>.\"\n",
    "completed_sentence = cloze_test(transformer, tokenizer, masked_sentence)\n",
    "print(\"Masked:\", masked_sentence)\n",
    "print(\"Completed:\", completed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Transformer' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 例: 文法誤り検出\u001b[39;00m\n\u001b[1;32m     12\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a correct sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShe like apples.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am go to school.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m ]\n\u001b[0;32m---> 18\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_grammar_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence, loss, is_correct \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Correct: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_correct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36mdetect_grammar_errors\u001b[0;34m(model, tokenizer, sentences, threshold)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[1;32m      5\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(sentence, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss_for_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     is_correct \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m<\u001b[39m threshold\n\u001b[1;32m      8\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((sentence, loss, is_correct))\n",
      "File \u001b[0;32m~/code/LLM_char.based-tokenization/notebook/GPT2.py:211\u001b[0m, in \u001b[0;36mTransformer.compute_loss_for_sentence\u001b[0;34m(self, sentence_tokens, tokenizer)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss_for_sentence\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_tokens, tokenizer):\n\u001b[1;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Computes the loss for a given sentence.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    loss: The average loss value for the given sentence.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sentence_tokens, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m    212\u001b[0m     targets \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mclone()  \u001b[38;5;66;03m# In language modeling, target is usually the same sequence shifted by 1.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# Shift targets for next-token prediction\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Transformer' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "# 文法誤り検出\n",
    "def detect_grammar_errors(model, tokenizer, sentences, threshold=5.0):\n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        token_ids = tokenizer.encode(sentence, return_tensors=\"pt\").to(device)\n",
    "        loss = model.compute_loss_for_sentence(token_ids[0].tolist(), tokenizer)\n",
    "        is_correct = loss < threshold\n",
    "        results.append((sentence, loss, is_correct))\n",
    "    return results\n",
    "\n",
    "# 例: 文法誤り検出\n",
    "sentences = [\n",
    "    \"This is a correct sentence.\",\n",
    "    \"She like apples.\",\n",
    "    \"I am go to school.\"\n",
    "]\n",
    "\n",
    "results = detect_grammar_errors(transformer, tokenizer, sentences)\n",
    "for sentence, loss, is_correct in results:\n",
    "    print(f\"Sentence: '{sentence}', Loss: {loss:.2f}, Correct: {is_correct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成文法評価"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
