{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransformerとTokenizerは.pyで保存済みなため学習ループの実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トランスフォーマーの総パラメータ数: 85.63M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from GPT2 import Transformer, ModelConfig\n",
    "from Tokenizer import Tokenizer\n",
    "\n",
    "# データセットの作成\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.tokens = self.tokenizer.encode(text, eot=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.tokens[idx:idx+self.block_size]), torch.tensor(self.tokens[idx+1:idx+self.block_size+1])\n",
    "\n",
    "# データセットの読み込み\n",
    "# 相対パスを指定してテキストファイルを読み込む\n",
    "file_path1 = \"../data/ptb.train.txt\"\n",
    "file_path2 = \"../data/ptb.valid.txt\"\n",
    "file_path3 = \"../data/ptb.test.txt\"\n",
    "\n",
    "# ファイルを開いて内容を読み込む\n",
    "with open(file_path1, \"r\", encoding=\"utf-8\") as file:\n",
    "    ptb_train_text = file.read()\n",
    "\n",
    "with open(file_path2, \"r\", encoding=\"utf-8\") as file:\n",
    "    ptb_valid_text = file.read()\n",
    "\n",
    "with open(file_path3, \"r\", encoding=\"utf-8\") as file:\n",
    "    ptb_test_text = file.read()\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizerの初期化\n",
    "unique_chars_in_train_text = sorted(list(set(ptb_train_text)))\n",
    "tokenizer = Tokenizer(unique_chars_in_train_text)\n",
    "\n",
    "# モデルの初期化\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "block_size = 128\n",
    "vocab_size = len(tokenizer.str_to_idx)\n",
    "config = ModelConfig(\n",
    "    block_size=128,    # シーケンス長\n",
    "    vocab_size= vocab_size,  # ボキャブラリサイズ（例: GPTのボキャブラリ）\n",
    "    n_layer=12,        # 層数\n",
    "    n_embd=768,        # 埋め込み次元\n",
    "    n_head=32,         # アテンションヘッド\n",
    "    dropout=0.1        # ドロップアウト率\n",
    ")\n",
    "\n",
    "transformer = Transformer(config).to(device)\n",
    "\n",
    "# データセットの作成\n",
    "train_dataset = TextDataset(ptb_train_text, tokenizer, block_size)\n",
    "valid_dataset = TextDataset(ptb_valid_text, tokenizer, block_size)\n",
    "test_dataset = TextDataset(ptb_test_text, tokenizer, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# 学習ループの実装\n",
    "def train(model, dataloader, optimizer, epochs, device):\n",
    "    model.train()  # モデルを訓練モードに設定\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (input_ids, target_ids) in enumerate(dataloader):\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            # モデルの順伝播\n",
    "            logits, loss, attention = model(input_ids, targets=target_ids)\n",
    "\n",
    "            # 損失の計算と逆伝播\n",
    "            optimizer.zero_grad()  # 勾配の初期化\n",
    "            loss.backward()  # 逆伝播\n",
    "            optimizer.step()  # パラメータ更新\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if i % 100 == 99:  # 100ステップごとに進捗を表示\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], Loss: {running_loss / 100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print(\"学習完了\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/2550746], Loss: 3.1580\n",
      "Epoch [1/1], Step [200/2550746], Loss: 3.0205\n",
      "Epoch [1/1], Step [300/2550746], Loss: 3.0191\n",
      "Epoch [1/1], Step [400/2550746], Loss: 3.0031\n",
      "Epoch [1/1], Step [500/2550746], Loss: 3.0212\n",
      "Epoch [1/1], Step [600/2550746], Loss: 3.0027\n",
      "Epoch [1/1], Step [700/2550746], Loss: 3.0189\n",
      "Epoch [1/1], Step [800/2550746], Loss: 3.0154\n",
      "Epoch [1/1], Step [900/2550746], Loss: 3.0060\n",
      "Epoch [1/1], Step [1000/2550746], Loss: 3.0006\n",
      "Epoch [1/1], Step [1100/2550746], Loss: 3.0133\n",
      "Epoch [1/1], Step [1200/2550746], Loss: 3.0021\n",
      "Epoch [1/1], Step [1300/2550746], Loss: 2.9950\n",
      "Epoch [1/1], Step [1400/2550746], Loss: 2.9991\n",
      "Epoch [1/1], Step [1500/2550746], Loss: 3.0043\n",
      "Epoch [1/1], Step [1600/2550746], Loss: 3.0055\n",
      "Epoch [1/1], Step [1700/2550746], Loss: 2.9967\n",
      "Epoch [1/1], Step [1800/2550746], Loss: 2.9979\n",
      "Epoch [1/1], Step [1900/2550746], Loss: 2.9987\n",
      "Epoch [1/1], Step [2000/2550746], Loss: 2.9943\n",
      "Epoch [1/1], Step [2100/2550746], Loss: 3.0035\n",
      "Epoch [1/1], Step [2200/2550746], Loss: 2.9965\n",
      "Epoch [1/1], Step [2300/2550746], Loss: 3.0004\n",
      "Epoch [1/1], Step [2400/2550746], Loss: 2.9979\n",
      "Epoch [1/1], Step [2500/2550746], Loss: 2.9980\n",
      "Epoch [1/1], Step [2600/2550746], Loss: 2.9973\n",
      "Epoch [1/1], Step [2700/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [2800/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [2900/2550746], Loss: 2.9984\n",
      "Epoch [1/1], Step [3000/2550746], Loss: 2.9977\n",
      "Epoch [1/1], Step [3100/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [3200/2550746], Loss: 2.9948\n",
      "Epoch [1/1], Step [3300/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [3400/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [3500/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [3600/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [3700/2550746], Loss: 2.9930\n",
      "Epoch [1/1], Step [3800/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [3900/2550746], Loss: 2.9968\n",
      "Epoch [1/1], Step [4000/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [4100/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [4200/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [4300/2550746], Loss: 2.9945\n",
      "Epoch [1/1], Step [4400/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [4500/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [4600/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [4700/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [4800/2550746], Loss: 2.9965\n",
      "Epoch [1/1], Step [4900/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [5000/2550746], Loss: 2.9925\n",
      "Epoch [1/1], Step [5100/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [5200/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [5300/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [5400/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [5500/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [5600/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [5700/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [5800/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [5900/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [6000/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [6100/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [6200/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [6300/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [6400/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [6500/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [6600/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [6700/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [6800/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [6900/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [7000/2550746], Loss: 2.9958\n",
      "Epoch [1/1], Step [7100/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [7200/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [7300/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [7400/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [7500/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [7600/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [7700/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [7800/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [7900/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [8000/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [8100/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [8200/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [8300/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [8400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [8500/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [8600/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [8700/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [8800/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [8900/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [9000/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [9100/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [9200/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [9300/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [9400/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [9500/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [9600/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [9700/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [9800/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [9900/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [10000/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [10100/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [10200/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [10300/2550746], Loss: 2.9930\n",
      "Epoch [1/1], Step [10400/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [10500/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [10600/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [10700/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [10800/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [10900/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [11000/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [11100/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [11200/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [11300/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [11400/2550746], Loss: 2.9995\n",
      "Epoch [1/1], Step [11500/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [11600/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [11700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [11800/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [11900/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [12000/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [12100/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [12200/2550746], Loss: 2.9953\n",
      "Epoch [1/1], Step [12300/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [12400/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [12500/2550746], Loss: 2.9954\n",
      "Epoch [1/1], Step [12600/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [12700/2550746], Loss: 2.9919\n",
      "Epoch [1/1], Step [12800/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [12900/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [13000/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [13100/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [13200/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [13300/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [13400/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [13500/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [13600/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [13700/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [13800/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [13900/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [14000/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [14100/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [14200/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [14300/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [14400/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [14500/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [14600/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [14700/2550746], Loss: 2.9937\n",
      "Epoch [1/1], Step [14800/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [14900/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [15000/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [15100/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [15200/2550746], Loss: 2.9925\n",
      "Epoch [1/1], Step [15300/2550746], Loss: 2.9723\n",
      "Epoch [1/1], Step [15400/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [15500/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [15600/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [15700/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [15800/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [15900/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [16000/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [16100/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [16200/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [16300/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [16400/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [16500/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [16600/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [16700/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [16800/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [16900/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [17000/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [17100/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [17200/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [17300/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [17400/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [17500/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [17600/2550746], Loss: 2.9969\n",
      "Epoch [1/1], Step [17700/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [17800/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [17900/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [18000/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [18100/2550746], Loss: 2.9988\n",
      "Epoch [1/1], Step [18200/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [18300/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [18400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [18500/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [18600/2550746], Loss: 2.9961\n",
      "Epoch [1/1], Step [18700/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [18800/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [18900/2550746], Loss: 2.9979\n",
      "Epoch [1/1], Step [19000/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [19100/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [19200/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [19300/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [19400/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [19500/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [19600/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [19700/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [19800/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [19900/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [20000/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [20100/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [20200/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [20300/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [20400/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [20500/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [20600/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [20700/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [20800/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [20900/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [21000/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [21100/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [21200/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [21300/2550746], Loss: 2.9923\n",
      "Epoch [1/1], Step [21400/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [21500/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [21600/2550746], Loss: 2.9929\n",
      "Epoch [1/1], Step [21700/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [21800/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [21900/2550746], Loss: 2.9945\n",
      "Epoch [1/1], Step [22000/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [22100/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [22200/2550746], Loss: 2.9730\n",
      "Epoch [1/1], Step [22300/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [22400/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [22500/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [22600/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [22700/2550746], Loss: 2.9931\n",
      "Epoch [1/1], Step [22800/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [22900/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [23000/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [23100/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [23200/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [23300/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [23400/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [23500/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [23600/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [23700/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [23800/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [23900/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [24000/2550746], Loss: 2.9942\n",
      "Epoch [1/1], Step [24100/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [24200/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [24300/2550746], Loss: 2.9747\n",
      "Epoch [1/1], Step [24400/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [24500/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [24600/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [24700/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [24800/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [24900/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [25000/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [25100/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [25200/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [25300/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [25400/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [25500/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [25600/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [25700/2550746], Loss: 2.9951\n",
      "Epoch [1/1], Step [25800/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [25900/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [26000/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [26100/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [26200/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [26300/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [26400/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [26500/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [26600/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [26700/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [26800/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [26900/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [27000/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [27100/2550746], Loss: 2.9946\n",
      "Epoch [1/1], Step [27200/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [27300/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [27400/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [27500/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [27600/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [27700/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [27800/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [27900/2550746], Loss: 2.9943\n",
      "Epoch [1/1], Step [28000/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [28100/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [28200/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [28300/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [28400/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [28500/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [28600/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [28700/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [28800/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [28900/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [29000/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [29100/2550746], Loss: 2.9940\n",
      "Epoch [1/1], Step [29200/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [29300/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [29400/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [29500/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [29600/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [29700/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [29800/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [29900/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [30000/2550746], Loss: 2.9985\n",
      "Epoch [1/1], Step [30100/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [30200/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [30300/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [30400/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [30500/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [30600/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [30700/2550746], Loss: 2.9748\n",
      "Epoch [1/1], Step [30800/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [30900/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [31000/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [31100/2550746], Loss: 2.9925\n",
      "Epoch [1/1], Step [31200/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [31300/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [31400/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [31500/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [31600/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [31700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [31800/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [31900/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [32000/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [32100/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [32200/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [32300/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [32400/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [32500/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [32600/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [32700/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [32800/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [32900/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [33000/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [33100/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [33200/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [33300/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [33400/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [33500/2550746], Loss: 2.9945\n",
      "Epoch [1/1], Step [33600/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [33700/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [33800/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [33900/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [34000/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [34100/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [34200/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [34300/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [34400/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [34500/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [34600/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [34700/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [34800/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [34900/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [35000/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [35100/2550746], Loss: 2.9707\n",
      "Epoch [1/1], Step [35200/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [35300/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [35400/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [35500/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [35600/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [35700/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [35800/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [35900/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [36000/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [36100/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [36200/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [36300/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [36400/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [36500/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [36600/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [36700/2550746], Loss: 2.9962\n",
      "Epoch [1/1], Step [36800/2550746], Loss: 2.9956\n",
      "Epoch [1/1], Step [36900/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [37000/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [37100/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [37200/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [37300/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [37400/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [37500/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [37600/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [37700/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [37800/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [37900/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [38000/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [38100/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [38200/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [38300/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [38400/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [38500/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [38600/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [38700/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [38800/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [38900/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [39000/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [39100/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [39200/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [39300/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [39400/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [39500/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [39600/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [39700/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [39800/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [39900/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [40000/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [40100/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [40200/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [40300/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [40400/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [40500/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [40600/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [40700/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [40800/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [40900/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [41000/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [41100/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [41200/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [41300/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [41400/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [41500/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [41600/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [41700/2550746], Loss: 2.9738\n",
      "Epoch [1/1], Step [41800/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [41900/2550746], Loss: 2.9729\n",
      "Epoch [1/1], Step [42000/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [42100/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [42200/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [42300/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [42400/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [42500/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [42600/2550746], Loss: 2.9909\n",
      "Epoch [1/1], Step [42700/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [42800/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [42900/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [43000/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [43100/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [43200/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [43300/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [43400/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [43500/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [43600/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [43700/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [43800/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [43900/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [44000/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [44100/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [44200/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [44300/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [44400/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [44500/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [44600/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [44700/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [44800/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [44900/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [45000/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [45100/2550746], Loss: 2.9729\n",
      "Epoch [1/1], Step [45200/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [45300/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [45400/2550746], Loss: 2.9938\n",
      "Epoch [1/1], Step [45500/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [45600/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [45700/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [45800/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [45900/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [46000/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [46100/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [46200/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [46300/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [46400/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [46500/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [46600/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [46700/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [46800/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [46900/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [47000/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [47100/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [47200/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [47300/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [47400/2550746], Loss: 2.9923\n",
      "Epoch [1/1], Step [47500/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [47600/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [47700/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [47800/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [47900/2550746], Loss: 2.9749\n",
      "Epoch [1/1], Step [48000/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [48100/2550746], Loss: 2.9931\n",
      "Epoch [1/1], Step [48200/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [48300/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [48400/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [48500/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [48600/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [48700/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [48800/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [48900/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [49000/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [49100/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [49200/2550746], Loss: 2.9745\n",
      "Epoch [1/1], Step [49300/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [49400/2550746], Loss: 2.9937\n",
      "Epoch [1/1], Step [49500/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [49600/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [49700/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [49800/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [49900/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [50000/2550746], Loss: 2.9953\n",
      "Epoch [1/1], Step [50100/2550746], Loss: 2.9956\n",
      "Epoch [1/1], Step [50200/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [50300/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [50400/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [50500/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [50600/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [50700/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [50800/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [50900/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [51000/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [51100/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [51200/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [51300/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [51400/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [51500/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [51600/2550746], Loss: 3.0011\n",
      "Epoch [1/1], Step [51700/2550746], Loss: 2.9696\n",
      "Epoch [1/1], Step [51800/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [51900/2550746], Loss: 2.9944\n",
      "Epoch [1/1], Step [52000/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [52100/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [52200/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [52300/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [52400/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [52500/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [52600/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [52700/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [52800/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [52900/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [53000/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [53100/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [53200/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [53300/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [53400/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [53500/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [53600/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [53700/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [53800/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [53900/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [54000/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [54100/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [54200/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [54300/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [54400/2550746], Loss: 2.9706\n",
      "Epoch [1/1], Step [54500/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [54600/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [54700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [54800/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [54900/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [55000/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [55100/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [55200/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [55300/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [55400/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [55500/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [55600/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [55700/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [55800/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [55900/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [56000/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [56100/2550746], Loss: 2.9919\n",
      "Epoch [1/1], Step [56200/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [56300/2550746], Loss: 2.9929\n",
      "Epoch [1/1], Step [56400/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [56500/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [56600/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [56700/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [56800/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [56900/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [57000/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [57100/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [57200/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [57300/2550746], Loss: 2.9754\n",
      "Epoch [1/1], Step [57400/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [57500/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [57600/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [57700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [57800/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [57900/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [58000/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [58100/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [58200/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [58300/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [58400/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [58500/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [58600/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [58700/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [58800/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [58900/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [59000/2550746], Loss: 2.9699\n",
      "Epoch [1/1], Step [59100/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [59200/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [59300/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [59400/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [59500/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [59600/2550746], Loss: 2.9741\n",
      "Epoch [1/1], Step [59700/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [59800/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [59900/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [60000/2550746], Loss: 2.9942\n",
      "Epoch [1/1], Step [60100/2550746], Loss: 2.9948\n",
      "Epoch [1/1], Step [60200/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [60300/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [60400/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [60500/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [60600/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [60700/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [60800/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [60900/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [61000/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [61100/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [61200/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [61300/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [61400/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [61500/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [61600/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [61700/2550746], Loss: 2.9755\n",
      "Epoch [1/1], Step [61800/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [61900/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [62000/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [62100/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [62200/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [62300/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [62400/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [62500/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [62600/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [62700/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [62800/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [62900/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [63000/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [63100/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [63200/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [63300/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [63400/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [63500/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [63600/2550746], Loss: 2.9756\n",
      "Epoch [1/1], Step [63700/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [63800/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [63900/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [64000/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [64100/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [64200/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [64300/2550746], Loss: 2.9995\n",
      "Epoch [1/1], Step [64400/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [64500/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [64600/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [64700/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [64800/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [64900/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [65000/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [65100/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [65200/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [65300/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [65400/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [65500/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [65600/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [65700/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [65800/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [65900/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [66000/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [66100/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [66200/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [66300/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [66400/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [66500/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [66600/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [66700/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [66800/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [66900/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [67000/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [67100/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [67200/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [67300/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [67400/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [67500/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [67600/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [67700/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [67800/2550746], Loss: 2.9948\n",
      "Epoch [1/1], Step [67900/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [68000/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [68100/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [68200/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [68300/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [68400/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [68500/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [68600/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [68700/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [68800/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [68900/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [69000/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [69100/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [69200/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [69300/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [69400/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [69500/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [69600/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [69700/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [69800/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [69900/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [70000/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [70100/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [70200/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [70300/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [70400/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [70500/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [70600/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [70700/2550746], Loss: 2.9962\n",
      "Epoch [1/1], Step [70800/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [70900/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [71000/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [71100/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [71200/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [71300/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [71400/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [71500/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [71600/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [71700/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [71800/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [71900/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [72000/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [72100/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [72200/2550746], Loss: 2.9967\n",
      "Epoch [1/1], Step [72300/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [72400/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [72500/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [72600/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [72700/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [72800/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [72900/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [73000/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [73100/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [73200/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [73300/2550746], Loss: 2.9919\n",
      "Epoch [1/1], Step [73400/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [73500/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [73600/2550746], Loss: 2.9754\n",
      "Epoch [1/1], Step [73700/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [73800/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [73900/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [74000/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [74100/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [74200/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [74300/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [74400/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [74500/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [74600/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [74700/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [74800/2550746], Loss: 2.9961\n",
      "Epoch [1/1], Step [74900/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [75000/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [75100/2550746], Loss: 2.9738\n",
      "Epoch [1/1], Step [75200/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [75300/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [75400/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [75500/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [75600/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [75700/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [75800/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [75900/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [76000/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [76100/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [76200/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [76300/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [76400/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [76500/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [76600/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [76700/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [76800/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [76900/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [77000/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [77100/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [77200/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [77300/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [77400/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [77500/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [77600/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [77700/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [77800/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [77900/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [78000/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [78100/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [78200/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [78300/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [78400/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [78500/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [78600/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [78700/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [78800/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [78900/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [79000/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [79100/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [79200/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [79300/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [79400/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [79500/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [79600/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [79700/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [79800/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [79900/2550746], Loss: 2.9931\n",
      "Epoch [1/1], Step [80000/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [80100/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [80200/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [80300/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [80400/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [80500/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [80600/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [80700/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [80800/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [80900/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [81000/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [81100/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [81200/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [81300/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [81400/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [81500/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [81600/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [81700/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [81800/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [81900/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [82000/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [82100/2550746], Loss: 2.9749\n",
      "Epoch [1/1], Step [82200/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [82300/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [82400/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [82500/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [82600/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [82700/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [82800/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [82900/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [83000/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [83100/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [83200/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [83300/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [83400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [83500/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [83600/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [83700/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [83800/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [83900/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [84000/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [84100/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [84200/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [84300/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [84400/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [84500/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [84600/2550746], Loss: 2.9748\n",
      "Epoch [1/1], Step [84700/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [84800/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [84900/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [85000/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [85100/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [85200/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [85300/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [85400/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [85500/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [85600/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [85700/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [85800/2550746], Loss: 2.9726\n",
      "Epoch [1/1], Step [85900/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [86000/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [86100/2550746], Loss: 2.9954\n",
      "Epoch [1/1], Step [86200/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [86300/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [86400/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [86500/2550746], Loss: 2.9953\n",
      "Epoch [1/1], Step [86600/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [86700/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [86800/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [86900/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [87000/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [87100/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [87200/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [87300/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [87400/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [87500/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [87600/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [87700/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [87800/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [87900/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [88000/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [88100/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [88200/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [88300/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [88400/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [88500/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [88600/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [88700/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [88800/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [88900/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [89000/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [89100/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [89200/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [89300/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [89400/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [89500/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [89600/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [89700/2550746], Loss: 2.9928\n",
      "Epoch [1/1], Step [89800/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [89900/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [90000/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [90100/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [90200/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [90300/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [90400/2550746], Loss: 2.9909\n",
      "Epoch [1/1], Step [90500/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [90600/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [90700/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [90800/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [90900/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [91000/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [91100/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [91200/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [91300/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [91400/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [91500/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [91600/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [91700/2550746], Loss: 2.9952\n",
      "Epoch [1/1], Step [91800/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [91900/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [92000/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [92100/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [92200/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [92300/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [92400/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [92500/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [92600/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [92700/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [92800/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [92900/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [93000/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [93100/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [93200/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [93300/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [93400/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [93500/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [93600/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [93700/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [93800/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [93900/2550746], Loss: 2.9749\n",
      "Epoch [1/1], Step [94000/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [94100/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [94200/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [94300/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [94400/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [94500/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [94600/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [94700/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [94800/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [94900/2550746], Loss: 2.9937\n",
      "Epoch [1/1], Step [95000/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [95100/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [95200/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [95300/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [95400/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [95500/2550746], Loss: 2.9961\n",
      "Epoch [1/1], Step [95600/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [95700/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [95800/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [95900/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [96000/2550746], Loss: 2.9733\n",
      "Epoch [1/1], Step [96100/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [96200/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [96300/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [96400/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [96500/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [96600/2550746], Loss: 2.9740\n",
      "Epoch [1/1], Step [96700/2550746], Loss: 2.9756\n",
      "Epoch [1/1], Step [96800/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [96900/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [97000/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [97100/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [97200/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [97300/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [97400/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [97500/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [97600/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [97700/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [97800/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [97900/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [98000/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [98100/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [98200/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [98300/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [98400/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [98500/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [98600/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [98700/2550746], Loss: 2.9716\n",
      "Epoch [1/1], Step [98800/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [98900/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [99000/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [99100/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [99200/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [99300/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [99400/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [99500/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [99600/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [99700/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [99800/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [99900/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [100000/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [100100/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [100200/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [100300/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [100400/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [100500/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [100600/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [100700/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [100800/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [100900/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [101000/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [101100/2550746], Loss: 2.9929\n",
      "Epoch [1/1], Step [101200/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [101300/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [101400/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [101500/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [101600/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [101700/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [101800/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [101900/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [102000/2550746], Loss: 2.9989\n",
      "Epoch [1/1], Step [102100/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [102200/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [102300/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [102400/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [102500/2550746], Loss: 2.9731\n",
      "Epoch [1/1], Step [102600/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [102700/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [102800/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [102900/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [103000/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [103100/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [103200/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [103300/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [103400/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [103500/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [103600/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [103700/2550746], Loss: 2.9925\n",
      "Epoch [1/1], Step [103800/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [103900/2550746], Loss: 2.9919\n",
      "Epoch [1/1], Step [104000/2550746], Loss: 2.9931\n",
      "Epoch [1/1], Step [104100/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [104200/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [104300/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [104400/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [104500/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [104600/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [104700/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [104800/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [104900/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [105000/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [105100/2550746], Loss: 2.9684\n",
      "Epoch [1/1], Step [105200/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [105300/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [105400/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [105500/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [105600/2550746], Loss: 2.9983\n",
      "Epoch [1/1], Step [105700/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [105800/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [105900/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [106000/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [106100/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [106200/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [106300/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [106400/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [106500/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [106600/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [106700/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [106800/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [106900/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [107000/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [107100/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [107200/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [107300/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [107400/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [107500/2550746], Loss: 2.9742\n",
      "Epoch [1/1], Step [107600/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [107700/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [107800/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [107900/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [108000/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [108100/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [108200/2550746], Loss: 2.9730\n",
      "Epoch [1/1], Step [108300/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [108400/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [108500/2550746], Loss: 2.9731\n",
      "Epoch [1/1], Step [108600/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [108700/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [108800/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [108900/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [109000/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [109100/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [109200/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [109300/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [109400/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [109500/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [109600/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [109700/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [109800/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [109900/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [110000/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [110100/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [110200/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [110300/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [110400/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [110500/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [110600/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [110700/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [110800/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [110900/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [111000/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [111100/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [111200/2550746], Loss: 2.9720\n",
      "Epoch [1/1], Step [111300/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [111400/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [111500/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [111600/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [111700/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [111800/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [111900/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [112000/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [112100/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [112200/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [112300/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [112400/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [112500/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [112600/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [112700/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [112800/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [112900/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [113000/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [113100/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [113200/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [113300/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [113400/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [113500/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [113600/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [113700/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [113800/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [113900/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [114000/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [114100/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [114200/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [114300/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [114400/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [114500/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [114600/2550746], Loss: 2.9939\n",
      "Epoch [1/1], Step [114700/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [114800/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [114900/2550746], Loss: 2.9735\n",
      "Epoch [1/1], Step [115000/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [115100/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [115200/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [115300/2550746], Loss: 3.0001\n",
      "Epoch [1/1], Step [115400/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [115500/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [115600/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [115700/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [115800/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [115900/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [116000/2550746], Loss: 2.9938\n",
      "Epoch [1/1], Step [116100/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [116200/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [116300/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [116400/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [116500/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [116600/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [116700/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [116800/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [116900/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [117000/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [117100/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [117200/2550746], Loss: 2.9940\n",
      "Epoch [1/1], Step [117300/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [117400/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [117500/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [117600/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [117700/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [117800/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [117900/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [118000/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [118100/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [118200/2550746], Loss: 2.9751\n",
      "Epoch [1/1], Step [118300/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [118400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [118500/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [118600/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [118700/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [118800/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [118900/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [119000/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [119100/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [119200/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [119300/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [119400/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [119500/2550746], Loss: 2.9729\n",
      "Epoch [1/1], Step [119600/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [119700/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [119800/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [119900/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [120000/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [120100/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [120200/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [120300/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [120400/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [120500/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [120600/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [120700/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [120800/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [120900/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [121000/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [121100/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [121200/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [121300/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [121400/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [121500/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [121600/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [121700/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [121800/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [121900/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [122000/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [122100/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [122200/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [122300/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [122400/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [122500/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [122600/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [122700/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [122800/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [122900/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [123000/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [123100/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [123200/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [123300/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [123400/2550746], Loss: 2.9938\n",
      "Epoch [1/1], Step [123500/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [123600/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [123700/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [123800/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [123900/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [124000/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [124100/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [124200/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [124300/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [124400/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [124500/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [124600/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [124700/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [124800/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [124900/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [125000/2550746], Loss: 2.9751\n",
      "Epoch [1/1], Step [125100/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [125200/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [125300/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [125400/2550746], Loss: 2.9909\n",
      "Epoch [1/1], Step [125500/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [125600/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [125700/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [125800/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [125900/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [126000/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [126100/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [126200/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [126300/2550746], Loss: 2.9927\n",
      "Epoch [1/1], Step [126400/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [126500/2550746], Loss: 2.9766\n",
      "Epoch [1/1], Step [126600/2550746], Loss: 2.9730\n",
      "Epoch [1/1], Step [126700/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [126800/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [126900/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [127000/2550746], Loss: 2.9753\n",
      "Epoch [1/1], Step [127100/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [127200/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [127300/2550746], Loss: 2.9948\n",
      "Epoch [1/1], Step [127400/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [127500/2550746], Loss: 2.9909\n",
      "Epoch [1/1], Step [127600/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [127700/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [127800/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [127900/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [128000/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [128100/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [128200/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [128300/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [128400/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [128500/2550746], Loss: 2.9923\n",
      "Epoch [1/1], Step [128600/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [128700/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [128800/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [128900/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [129000/2550746], Loss: 2.9718\n",
      "Epoch [1/1], Step [129100/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [129200/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [129300/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [129400/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [129500/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [129600/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [129700/2550746], Loss: 2.9730\n",
      "Epoch [1/1], Step [129800/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [129900/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [130000/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [130100/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [130200/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [130300/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [130400/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [130500/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [130600/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [130700/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [130800/2550746], Loss: 2.9709\n",
      "Epoch [1/1], Step [130900/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [131000/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [131100/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [131200/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [131300/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [131400/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [131500/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [131600/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [131700/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [131800/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [131900/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [132000/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [132100/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [132200/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [132300/2550746], Loss: 2.9946\n",
      "Epoch [1/1], Step [132400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [132500/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [132600/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [132700/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [132800/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [132900/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [133000/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [133100/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [133200/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [133300/2550746], Loss: 2.9726\n",
      "Epoch [1/1], Step [133400/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [133500/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [133600/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [133700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [133800/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [133900/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [134000/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [134100/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [134200/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [134300/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [134400/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [134500/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [134600/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [134700/2550746], Loss: 2.9923\n",
      "Epoch [1/1], Step [134800/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [134900/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [135000/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [135100/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [135200/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [135300/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [135400/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [135500/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [135600/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [135700/2550746], Loss: 2.9755\n",
      "Epoch [1/1], Step [135800/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [135900/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [136000/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [136100/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [136200/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [136300/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [136400/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [136500/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [136600/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [136700/2550746], Loss: 2.9943\n",
      "Epoch [1/1], Step [136800/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [136900/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [137000/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [137100/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [137200/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [137300/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [137400/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [137500/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [137600/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [137700/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [137800/2550746], Loss: 2.9740\n",
      "Epoch [1/1], Step [137900/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [138000/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [138100/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [138200/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [138300/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [138400/2550746], Loss: 2.9701\n",
      "Epoch [1/1], Step [138500/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [138600/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [138700/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [138800/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [138900/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [139000/2550746], Loss: 2.9753\n",
      "Epoch [1/1], Step [139100/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [139200/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [139300/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [139400/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [139500/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [139600/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [139700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [139800/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [139900/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [140000/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [140100/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [140200/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [140300/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [140400/2550746], Loss: 2.9730\n",
      "Epoch [1/1], Step [140500/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [140600/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [140700/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [140800/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [140900/2550746], Loss: 2.9754\n",
      "Epoch [1/1], Step [141000/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [141100/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [141200/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [141300/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [141400/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [141500/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [141600/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [141700/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [141800/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [141900/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [142000/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [142100/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [142200/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [142300/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [142400/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [142500/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [142600/2550746], Loss: 2.9927\n",
      "Epoch [1/1], Step [142700/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [142800/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [142900/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [143000/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [143100/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [143200/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [143300/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [143400/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [143500/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [143600/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [143700/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [143800/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [143900/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [144000/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [144100/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [144200/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [144300/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [144400/2550746], Loss: 2.9756\n",
      "Epoch [1/1], Step [144500/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [144600/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [144700/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [144800/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [144900/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [145000/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [145100/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [145200/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [145300/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [145400/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [145500/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [145600/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [145700/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [145800/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [145900/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [146000/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [146100/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [146200/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [146300/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [146400/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [146500/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [146600/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [146700/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [146800/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [146900/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [147000/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [147100/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [147200/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [147300/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [147400/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [147500/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [147600/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [147700/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [147800/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [147900/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [148000/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [148100/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [148200/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [148300/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [148400/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [148500/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [148600/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [148700/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [148800/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [148900/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [149000/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [149100/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [149200/2550746], Loss: 2.9972\n",
      "Epoch [1/1], Step [149300/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [149400/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [149500/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [149600/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [149700/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [149800/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [149900/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [150000/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [150100/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [150200/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [150300/2550746], Loss: 2.9928\n",
      "Epoch [1/1], Step [150400/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [150500/2550746], Loss: 2.9928\n",
      "Epoch [1/1], Step [150600/2550746], Loss: 2.9929\n",
      "Epoch [1/1], Step [150700/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [150800/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [150900/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [151000/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [151100/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [151200/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [151300/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [151400/2550746], Loss: 2.9740\n",
      "Epoch [1/1], Step [151500/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [151600/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [151700/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [151800/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [151900/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [152000/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [152100/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [152200/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [152300/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [152400/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [152500/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [152600/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [152700/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [152800/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [152900/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [153000/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [153100/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [153200/2550746], Loss: 2.9927\n",
      "Epoch [1/1], Step [153300/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [153400/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [153500/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [153600/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [153700/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [153800/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [153900/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [154000/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [154100/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [154200/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [154300/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [154400/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [154500/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [154600/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [154700/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [154800/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [154900/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [155000/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [155100/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [155200/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [155300/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [155400/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [155500/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [155600/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [155700/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [155800/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [155900/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [156000/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [156100/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [156200/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [156300/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [156400/2550746], Loss: 2.9747\n",
      "Epoch [1/1], Step [156500/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [156600/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [156700/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [156800/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [156900/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [157000/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [157100/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [157200/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [157300/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [157400/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [157500/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [157600/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [157700/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [157800/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [157900/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [158000/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [158100/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [158200/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [158300/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [158400/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [158500/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [158600/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [158700/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [158800/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [158900/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [159000/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [159100/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [159200/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [159300/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [159400/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [159500/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [159600/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [159700/2550746], Loss: 2.9744\n",
      "Epoch [1/1], Step [159800/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [159900/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [160000/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [160100/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [160200/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [160300/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [160400/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [160500/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [160600/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [160700/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [160800/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [160900/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [161000/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [161100/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [161200/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [161300/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [161400/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [161500/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [161600/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [161700/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [161800/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [161900/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [162000/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [162100/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [162200/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [162300/2550746], Loss: 2.9947\n",
      "Epoch [1/1], Step [162400/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [162500/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [162600/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [162700/2550746], Loss: 2.9947\n",
      "Epoch [1/1], Step [162800/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [162900/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [163000/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [163100/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [163200/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [163300/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [163400/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [163500/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [163600/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [163700/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [163800/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [163900/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [164000/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [164100/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [164200/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [164300/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [164400/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [164500/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [164600/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [164700/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [164800/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [164900/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [165000/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [165100/2550746], Loss: 2.9716\n",
      "Epoch [1/1], Step [165200/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [165300/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [165400/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [165500/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [165600/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [165700/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [165800/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [165900/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [166000/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [166100/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [166200/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [166300/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [166400/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [166500/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [166600/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [166700/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [166800/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [166900/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [167000/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [167100/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [167200/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [167300/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [167400/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [167500/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [167600/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [167700/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [167800/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [167900/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [168000/2550746], Loss: 2.9721\n",
      "Epoch [1/1], Step [168100/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [168200/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [168300/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [168400/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [168500/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [168600/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [168700/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [168800/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [168900/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [169000/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [169100/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [169200/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [169300/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [169400/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [169500/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [169600/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [169700/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [169800/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [169900/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [170000/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [170100/2550746], Loss: 2.9733\n",
      "Epoch [1/1], Step [170200/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [170300/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [170400/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [170500/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [170600/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [170700/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [170800/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [170900/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [171000/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [171100/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [171200/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [171300/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [171400/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [171500/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [171600/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [171700/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [171800/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [171900/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [172000/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [172100/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [172200/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [172300/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [172400/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [172500/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [172600/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [172700/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [172800/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [172900/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [173000/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [173100/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [173200/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [173300/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [173400/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [173500/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [173600/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [173700/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [173800/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [173900/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [174000/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [174100/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [174200/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [174300/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [174400/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [174500/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [174600/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [174700/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [174800/2550746], Loss: 2.9745\n",
      "Epoch [1/1], Step [174900/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [175000/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [175100/2550746], Loss: 2.9753\n",
      "Epoch [1/1], Step [175200/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [175300/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [175400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [175500/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [175600/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [175700/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [175800/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [175900/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [176000/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [176100/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [176200/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [176300/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [176400/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [176500/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [176600/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [176700/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [176800/2550746], Loss: 2.9940\n",
      "Epoch [1/1], Step [176900/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [177000/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [177100/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [177200/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [177300/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [177400/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [177500/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [177600/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [177700/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [177800/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [177900/2550746], Loss: 2.9754\n",
      "Epoch [1/1], Step [178000/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [178100/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [178200/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [178300/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [178400/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [178500/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [178600/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [178700/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [178800/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [178900/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [179000/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [179100/2550746], Loss: 2.9980\n",
      "Epoch [1/1], Step [179200/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [179300/2550746], Loss: 2.9734\n",
      "Epoch [1/1], Step [179400/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [179500/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [179600/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [179700/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [179800/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [179900/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [180000/2550746], Loss: 2.9736\n",
      "Epoch [1/1], Step [180100/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [180200/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [180300/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [180400/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [180500/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [180600/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [180700/2550746], Loss: 2.9756\n",
      "Epoch [1/1], Step [180800/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [180900/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [181000/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [181100/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [181200/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [181300/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [181400/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [181500/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [181600/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [181700/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [181800/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [181900/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [182000/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [182100/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [182200/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [182300/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [182400/2550746], Loss: 2.9940\n",
      "Epoch [1/1], Step [182500/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [182600/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [182700/2550746], Loss: 2.9925\n",
      "Epoch [1/1], Step [182800/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [182900/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [183000/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [183100/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [183200/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [183300/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [183400/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [183500/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [183600/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [183700/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [183800/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [183900/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [184000/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [184100/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [184200/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [184300/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [184400/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [184500/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [184600/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [184700/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [184800/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [184900/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [185000/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [185100/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [185200/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [185300/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [185400/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [185500/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [185600/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [185700/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [185800/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [185900/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [186000/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [186100/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [186200/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [186300/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [186400/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [186500/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [186600/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [186700/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [186800/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [186900/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [187000/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [187100/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [187200/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [187300/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [187400/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [187500/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [187600/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [187700/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [187800/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [187900/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [188000/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [188100/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [188200/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [188300/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [188400/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [188500/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [188600/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [188700/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [188800/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [188900/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [189000/2550746], Loss: 2.9715\n",
      "Epoch [1/1], Step [189100/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [189200/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [189300/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [189400/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [189500/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [189600/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [189700/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [189800/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [189900/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [190000/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [190100/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [190200/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [190300/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [190400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [190500/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [190600/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [190700/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [190800/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [190900/2550746], Loss: 2.9927\n",
      "Epoch [1/1], Step [191000/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [191100/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [191200/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [191300/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [191400/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [191500/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [191600/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [191700/2550746], Loss: 2.9979\n",
      "Epoch [1/1], Step [191800/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [191900/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [192000/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [192100/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [192200/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [192300/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [192400/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [192500/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [192600/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [192700/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [192800/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [192900/2550746], Loss: 2.9929\n",
      "Epoch [1/1], Step [193000/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [193100/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [193200/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [193300/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [193400/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [193500/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [193600/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [193700/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [193800/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [193900/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [194000/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [194100/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [194200/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [194300/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [194400/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [194500/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [194600/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [194700/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [194800/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [194900/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [195000/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [195100/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [195200/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [195300/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [195400/2550746], Loss: 2.9725\n",
      "Epoch [1/1], Step [195500/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [195600/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [195700/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [195800/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [195900/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [196000/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [196100/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [196200/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [196300/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [196400/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [196500/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [196600/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [196700/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [196800/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [196900/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [197000/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [197100/2550746], Loss: 2.9959\n",
      "Epoch [1/1], Step [197200/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [197300/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [197400/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [197500/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [197600/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [197700/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [197800/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [197900/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [198000/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [198100/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [198200/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [198300/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [198400/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [198500/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [198600/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [198700/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [198800/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [198900/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [199000/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [199100/2550746], Loss: 2.9947\n",
      "Epoch [1/1], Step [199200/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [199300/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [199400/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [199500/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [199600/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [199700/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [199800/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [199900/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [200000/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [200100/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [200200/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [200300/2550746], Loss: 2.9949\n",
      "Epoch [1/1], Step [200400/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [200500/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [200600/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [200700/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [200800/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [200900/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [201000/2550746], Loss: 2.9923\n",
      "Epoch [1/1], Step [201100/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [201200/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [201300/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [201400/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [201500/2550746], Loss: 2.9938\n",
      "Epoch [1/1], Step [201600/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [201700/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [201800/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [201900/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [202000/2550746], Loss: 2.9941\n",
      "Epoch [1/1], Step [202100/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [202200/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [202300/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [202400/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [202500/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [202600/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [202700/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [202800/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [202900/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [203000/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [203100/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [203200/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [203300/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [203400/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [203500/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [203600/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [203700/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [203800/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [203900/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [204000/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [204100/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [204200/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [204300/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [204400/2550746], Loss: 2.9689\n",
      "Epoch [1/1], Step [204500/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [204600/2550746], Loss: 2.9738\n",
      "Epoch [1/1], Step [204700/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [204800/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [204900/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [205000/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [205100/2550746], Loss: 2.9742\n",
      "Epoch [1/1], Step [205200/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [205300/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [205400/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [205500/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [205600/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [205700/2550746], Loss: 2.9729\n",
      "Epoch [1/1], Step [205800/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [205900/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [206000/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [206100/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [206200/2550746], Loss: 2.9702\n",
      "Epoch [1/1], Step [206300/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [206400/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [206500/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [206600/2550746], Loss: 2.9736\n",
      "Epoch [1/1], Step [206700/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [206800/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [206900/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [207000/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [207100/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [207200/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [207300/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [207400/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [207500/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [207600/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [207700/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [207800/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [207900/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [208000/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [208100/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [208200/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [208300/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [208400/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [208500/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [208600/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [208700/2550746], Loss: 2.9937\n",
      "Epoch [1/1], Step [208800/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [208900/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [209000/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [209100/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [209200/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [209300/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [209400/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [209500/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [209600/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [209700/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [209800/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [209900/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [210000/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [210100/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [210200/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [210300/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [210400/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [210500/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [210600/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [210700/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [210800/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [210900/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [211000/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [211100/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [211200/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [211300/2550746], Loss: 2.9942\n",
      "Epoch [1/1], Step [211400/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [211500/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [211600/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [211700/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [211800/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [211900/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [212000/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [212100/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [212200/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [212300/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [212400/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [212500/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [212600/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [212700/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [212800/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [212900/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [213000/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [213100/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [213200/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [213300/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [213400/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [213500/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [213600/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [213700/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [213800/2550746], Loss: 2.9751\n",
      "Epoch [1/1], Step [213900/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [214000/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [214100/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [214200/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [214300/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [214400/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [214500/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [214600/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [214700/2550746], Loss: 2.9723\n",
      "Epoch [1/1], Step [214800/2550746], Loss: 2.9927\n",
      "Epoch [1/1], Step [214900/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [215000/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [215100/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [215200/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [215300/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [215400/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [215500/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [215600/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [215700/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [215800/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [215900/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [216000/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [216100/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [216200/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [216300/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [216400/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [216500/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [216600/2550746], Loss: 2.9923\n",
      "Epoch [1/1], Step [216700/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [216800/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [216900/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [217000/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [217100/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [217200/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [217300/2550746], Loss: 2.9747\n",
      "Epoch [1/1], Step [217400/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [217500/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [217600/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [217700/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [217800/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [217900/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [218000/2550746], Loss: 2.9746\n",
      "Epoch [1/1], Step [218100/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [218200/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [218300/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [218400/2550746], Loss: 2.9720\n",
      "Epoch [1/1], Step [218500/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [218600/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [218700/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [218800/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [218900/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [219000/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [219100/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [219200/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [219300/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [219400/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [219500/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [219600/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [219700/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [219800/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [219900/2550746], Loss: 2.9753\n",
      "Epoch [1/1], Step [220000/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [220100/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [220200/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [220300/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [220400/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [220500/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [220600/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [220700/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [220800/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [220900/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [221000/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [221100/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [221200/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [221300/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [221400/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [221500/2550746], Loss: 2.9721\n",
      "Epoch [1/1], Step [221600/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [221700/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [221800/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [221900/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [222000/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [222100/2550746], Loss: 2.9945\n",
      "Epoch [1/1], Step [222200/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [222300/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [222400/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [222500/2550746], Loss: 2.9927\n",
      "Epoch [1/1], Step [222600/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [222700/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [222800/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [222900/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [223000/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [223100/2550746], Loss: 2.9754\n",
      "Epoch [1/1], Step [223200/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [223300/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [223400/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [223500/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [223600/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [223700/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [223800/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [223900/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [224000/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [224100/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [224200/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [224300/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [224400/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [224500/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [224600/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [224700/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [224800/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [224900/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [225000/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [225100/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [225200/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [225300/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [225400/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [225500/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [225600/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [225700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [225800/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [225900/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [226000/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [226100/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [226200/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [226300/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [226400/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [226500/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [226600/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [226700/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [226800/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [226900/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [227000/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [227100/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [227200/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [227300/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [227400/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [227500/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [227600/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [227700/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [227800/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [227900/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [228000/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [228100/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [228200/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [228300/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [228400/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [228500/2550746], Loss: 2.9948\n",
      "Epoch [1/1], Step [228600/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [228700/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [228800/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [228900/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [229000/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [229100/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [229200/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [229300/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [229400/2550746], Loss: 2.9938\n",
      "Epoch [1/1], Step [229500/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [229600/2550746], Loss: 2.9746\n",
      "Epoch [1/1], Step [229700/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [229800/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [229900/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [230000/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [230100/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [230200/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [230300/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [230400/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [230500/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [230600/2550746], Loss: 2.9938\n",
      "Epoch [1/1], Step [230700/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [230800/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [230900/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [231000/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [231100/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [231200/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [231300/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [231400/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [231500/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [231600/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [231700/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [231800/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [231900/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [232000/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [232100/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [232200/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [232300/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [232400/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [232500/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [232600/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [232700/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [232800/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [232900/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [233000/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [233100/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [233200/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [233300/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [233400/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [233500/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [233600/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [233700/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [233800/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [233900/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [234000/2550746], Loss: 2.9951\n",
      "Epoch [1/1], Step [234100/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [234200/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [234300/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [234400/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [234500/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [234600/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [234700/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [234800/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [234900/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [235000/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [235100/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [235200/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [235300/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [235400/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [235500/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [235600/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [235700/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [235800/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [235900/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [236000/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [236100/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [236200/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [236300/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [236400/2550746], Loss: 2.9931\n",
      "Epoch [1/1], Step [236500/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [236600/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [236700/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [236800/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [236900/2550746], Loss: 2.9744\n",
      "Epoch [1/1], Step [237000/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [237100/2550746], Loss: 2.9909\n",
      "Epoch [1/1], Step [237200/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [237300/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [237400/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [237500/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [237600/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [237700/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [237800/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [237900/2550746], Loss: 2.9930\n",
      "Epoch [1/1], Step [238000/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [238100/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [238200/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [238300/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [238400/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [238500/2550746], Loss: 2.9949\n",
      "Epoch [1/1], Step [238600/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [238700/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [238800/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [238900/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [239000/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [239100/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [239200/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [239300/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [239400/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [239500/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [239600/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [239700/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [239800/2550746], Loss: 2.9742\n",
      "Epoch [1/1], Step [239900/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [240000/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [240100/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [240200/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [240300/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [240400/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [240500/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [240600/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [240700/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [240800/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [240900/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [241000/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [241100/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [241200/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [241300/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [241400/2550746], Loss: 2.9747\n",
      "Epoch [1/1], Step [241500/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [241600/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [241700/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [241800/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [241900/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [242000/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [242100/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [242200/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [242300/2550746], Loss: 2.9923\n",
      "Epoch [1/1], Step [242400/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [242500/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [242600/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [242700/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [242800/2550746], Loss: 2.9951\n",
      "Epoch [1/1], Step [242900/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [243000/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [243100/2550746], Loss: 2.9745\n",
      "Epoch [1/1], Step [243200/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [243300/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [243400/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [243500/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [243600/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [243700/2550746], Loss: 2.9737\n",
      "Epoch [1/1], Step [243800/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [243900/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [244000/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [244100/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [244200/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [244300/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [244400/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [244500/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [244600/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [244700/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [244800/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [244900/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [245000/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [245100/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [245200/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [245300/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [245400/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [245500/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [245600/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [245700/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [245800/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [245900/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [246000/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [246100/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [246200/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [246300/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [246400/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [246500/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [246600/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [246700/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [246800/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [246900/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [247000/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [247100/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [247200/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [247300/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [247400/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [247500/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [247600/2550746], Loss: 2.9699\n",
      "Epoch [1/1], Step [247700/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [247800/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [247900/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [248000/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [248100/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [248200/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [248300/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [248400/2550746], Loss: 2.9751\n",
      "Epoch [1/1], Step [248500/2550746], Loss: 2.9745\n",
      "Epoch [1/1], Step [248600/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [248700/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [248800/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [248900/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [249000/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [249100/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [249200/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [249300/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [249400/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [249500/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [249600/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [249700/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [249800/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [249900/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [250000/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [250100/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [250200/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [250300/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [250400/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [250500/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [250600/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [250700/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [250800/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [250900/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [251000/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [251100/2550746], Loss: 3.0011\n",
      "Epoch [1/1], Step [251200/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [251300/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [251400/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [251500/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [251600/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [251700/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [251800/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [251900/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [252000/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [252100/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [252200/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [252300/2550746], Loss: 2.9950\n",
      "Epoch [1/1], Step [252400/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [252500/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [252600/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [252700/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [252800/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [252900/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [253000/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [253100/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [253200/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [253300/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [253400/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [253500/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [253600/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [253700/2550746], Loss: 2.9703\n",
      "Epoch [1/1], Step [253800/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [253900/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [254000/2550746], Loss: 2.9962\n",
      "Epoch [1/1], Step [254100/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [254200/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [254300/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [254400/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [254500/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [254600/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [254700/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [254800/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [254900/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [255000/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [255100/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [255200/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [255300/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [255400/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [255500/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [255600/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [255700/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [255800/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [255900/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [256000/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [256100/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [256200/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [256300/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [256400/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [256500/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [256600/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [256700/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [256800/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [256900/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [257000/2550746], Loss: 2.9970\n",
      "Epoch [1/1], Step [257100/2550746], Loss: 2.9748\n",
      "Epoch [1/1], Step [257200/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [257300/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [257400/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [257500/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [257600/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [257700/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [257800/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [257900/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [258000/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [258100/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [258200/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [258300/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [258400/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [258500/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [258600/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [258700/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [258800/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [258900/2550746], Loss: 2.9722\n",
      "Epoch [1/1], Step [259000/2550746], Loss: 2.9754\n",
      "Epoch [1/1], Step [259100/2550746], Loss: 2.9931\n",
      "Epoch [1/1], Step [259200/2550746], Loss: 2.9747\n",
      "Epoch [1/1], Step [259300/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [259400/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [259500/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [259600/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [259700/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [259800/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [259900/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [260000/2550746], Loss: 2.9940\n",
      "Epoch [1/1], Step [260100/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [260200/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [260300/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [260400/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [260500/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [260600/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [260700/2550746], Loss: 2.9742\n",
      "Epoch [1/1], Step [260800/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [260900/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [261000/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [261100/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [261200/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [261300/2550746], Loss: 2.9731\n",
      "Epoch [1/1], Step [261400/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [261500/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [261600/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [261700/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [261800/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [261900/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [262000/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [262100/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [262200/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [262300/2550746], Loss: 2.9687\n",
      "Epoch [1/1], Step [262400/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [262500/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [262600/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [262700/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [262800/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [262900/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [263000/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [263100/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [263200/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [263300/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [263400/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [263500/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [263600/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [263700/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [263800/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [263900/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [264000/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [264100/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [264200/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [264300/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [264400/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [264500/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [264600/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [264700/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [264800/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [264900/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [265000/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [265100/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [265200/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [265300/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [265400/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [265500/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [265600/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [265700/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [265800/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [265900/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [266000/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [266100/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [266200/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [266300/2550746], Loss: 2.9666\n",
      "Epoch [1/1], Step [266400/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [266500/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [266600/2550746], Loss: 2.9960\n",
      "Epoch [1/1], Step [266700/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [266800/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [266900/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [267000/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [267100/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [267200/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [267300/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [267400/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [267500/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [267600/2550746], Loss: 2.9692\n",
      "Epoch [1/1], Step [267700/2550746], Loss: 2.9732\n",
      "Epoch [1/1], Step [267800/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [267900/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [268000/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [268100/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [268200/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [268300/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [268400/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [268500/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [268600/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [268700/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [268800/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [268900/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [269000/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [269100/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [269200/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [269300/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [269400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [269500/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [269600/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [269700/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [269800/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [269900/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [270000/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [270100/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [270200/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [270300/2550746], Loss: 2.9716\n",
      "Epoch [1/1], Step [270400/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [270500/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [270600/2550746], Loss: 2.9722\n",
      "Epoch [1/1], Step [270700/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [270800/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [270900/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [271000/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [271100/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [271200/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [271300/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [271400/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [271500/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [271600/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [271700/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [271800/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [271900/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [272000/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [272100/2550746], Loss: 2.9950\n",
      "Epoch [1/1], Step [272200/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [272300/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [272400/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [272500/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [272600/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [272700/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [272800/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [272900/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [273000/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [273100/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [273200/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [273300/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [273400/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [273500/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [273600/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [273700/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [273800/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [273900/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [274000/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [274100/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [274200/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [274300/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [274400/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [274500/2550746], Loss: 2.9950\n",
      "Epoch [1/1], Step [274600/2550746], Loss: 2.9703\n",
      "Epoch [1/1], Step [274700/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [274800/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [274900/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [275000/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [275100/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [275200/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [275300/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [275400/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [275500/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [275600/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [275700/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [275800/2550746], Loss: 2.9960\n",
      "Epoch [1/1], Step [275900/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [276000/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [276100/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [276200/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [276300/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [276400/2550746], Loss: 2.9738\n",
      "Epoch [1/1], Step [276500/2550746], Loss: 2.9964\n",
      "Epoch [1/1], Step [276600/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [276700/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [276800/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [276900/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [277000/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [277100/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [277200/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [277300/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [277400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [277500/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [277600/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [277700/2550746], Loss: 2.9729\n",
      "Epoch [1/1], Step [277800/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [277900/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [278000/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [278100/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [278200/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [278300/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [278400/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [278500/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [278600/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [278700/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [278800/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [278900/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [279000/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [279100/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [279200/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [279300/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [279400/2550746], Loss: 2.9753\n",
      "Epoch [1/1], Step [279500/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [279600/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [279700/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [279800/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [279900/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [280000/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [280100/2550746], Loss: 2.9994\n",
      "Epoch [1/1], Step [280200/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [280300/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [280400/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [280500/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [280600/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [280700/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [280800/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [280900/2550746], Loss: 2.9734\n",
      "Epoch [1/1], Step [281000/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [281100/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [281200/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [281300/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [281400/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [281500/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [281600/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [281700/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [281800/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [281900/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [282000/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [282100/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [282200/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [282300/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [282400/2550746], Loss: 2.9923\n",
      "Epoch [1/1], Step [282500/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [282600/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [282700/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [282800/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [282900/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [283000/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [283100/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [283200/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [283300/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [283400/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [283500/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [283600/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [283700/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [283800/2550746], Loss: 2.9964\n",
      "Epoch [1/1], Step [283900/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [284000/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [284100/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [284200/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [284300/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [284400/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [284500/2550746], Loss: 2.9930\n",
      "Epoch [1/1], Step [284600/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [284700/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [284800/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [284900/2550746], Loss: 2.9939\n",
      "Epoch [1/1], Step [285000/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [285100/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [285200/2550746], Loss: 2.9755\n",
      "Epoch [1/1], Step [285300/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [285400/2550746], Loss: 2.9919\n",
      "Epoch [1/1], Step [285500/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [285600/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [285700/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [285800/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [285900/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [286000/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [286100/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [286200/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [286300/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [286400/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [286500/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [286600/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [286700/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [286800/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [286900/2550746], Loss: 2.9909\n",
      "Epoch [1/1], Step [287000/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [287100/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [287200/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [287300/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [287400/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [287500/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [287600/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [287700/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [287800/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [287900/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [288000/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [288100/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [288200/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [288300/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [288400/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [288500/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [288600/2550746], Loss: 2.9735\n",
      "Epoch [1/1], Step [288700/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [288800/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [288900/2550746], Loss: 2.9746\n",
      "Epoch [1/1], Step [289000/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [289100/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [289200/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [289300/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [289400/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [289500/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [289600/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [289700/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [289800/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [289900/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [290000/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [290100/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [290200/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [290300/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [290400/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [290500/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [290600/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [290700/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [290800/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [290900/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [291000/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [291100/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [291200/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [291300/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [291400/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [291500/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [291600/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [291700/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [291800/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [291900/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [292000/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [292100/2550746], Loss: 2.9727\n",
      "Epoch [1/1], Step [292200/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [292300/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [292400/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [292500/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [292600/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [292700/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [292800/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [292900/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [293000/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [293100/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [293200/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [293300/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [293400/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [293500/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [293600/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [293700/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [293800/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [293900/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [294000/2550746], Loss: 2.9930\n",
      "Epoch [1/1], Step [294100/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [294200/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [294300/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [294400/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [294500/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [294600/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [294700/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [294800/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [294900/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [295000/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [295100/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [295200/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [295300/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [295400/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [295500/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [295600/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [295700/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [295800/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [295900/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [296000/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [296100/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [296200/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [296300/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [296400/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [296500/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [296600/2550746], Loss: 2.9950\n",
      "Epoch [1/1], Step [296700/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [296800/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [296900/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [297000/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [297100/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [297200/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [297300/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [297400/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [297500/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [297600/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [297700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [297800/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [297900/2550746], Loss: 3.0004\n",
      "Epoch [1/1], Step [298000/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [298100/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [298200/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [298300/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [298400/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [298500/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [298600/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [298700/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [298800/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [298900/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [299000/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [299100/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [299200/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [299300/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [299400/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [299500/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [299600/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [299700/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [299800/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [299900/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [300000/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [300100/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [300200/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [300300/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [300400/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [300500/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [300600/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [300700/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [300800/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [300900/2550746], Loss: 2.9959\n",
      "Epoch [1/1], Step [301000/2550746], Loss: 2.9712\n",
      "Epoch [1/1], Step [301100/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [301200/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [301300/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [301400/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [301500/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [301600/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [301700/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [301800/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [301900/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [302000/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [302100/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [302200/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [302300/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [302400/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [302500/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [302600/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [302700/2550746], Loss: 2.9751\n",
      "Epoch [1/1], Step [302800/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [302900/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [303000/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [303100/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [303200/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [303300/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [303400/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [303500/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [303600/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [303700/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [303800/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [303900/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [304000/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [304100/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [304200/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [304300/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [304400/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [304500/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [304600/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [304700/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [304800/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [304900/2550746], Loss: 2.9709\n",
      "Epoch [1/1], Step [305000/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [305100/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [305200/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [305300/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [305400/2550746], Loss: 2.9751\n",
      "Epoch [1/1], Step [305500/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [305600/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [305700/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [305800/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [305900/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [306000/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [306100/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [306200/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [306300/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [306400/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [306500/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [306600/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [306700/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [306800/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [306900/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [307000/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [307100/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [307200/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [307300/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [307400/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [307500/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [307600/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [307700/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [307800/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [307900/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [308000/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [308100/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [308200/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [308300/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [308400/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [308500/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [308600/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [308700/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [308800/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [308900/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [309000/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [309100/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [309200/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [309300/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [309400/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [309500/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [309600/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [309700/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [309800/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [309900/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [310000/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [310100/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [310200/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [310300/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [310400/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [310500/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [310600/2550746], Loss: 2.9756\n",
      "Epoch [1/1], Step [310700/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [310800/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [310900/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [311000/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [311100/2550746], Loss: 2.9766\n",
      "Epoch [1/1], Step [311200/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [311300/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [311400/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [311500/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [311600/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [311700/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [311800/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [311900/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [312000/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [312100/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [312200/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [312300/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [312400/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [312500/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [312600/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [312700/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [312800/2550746], Loss: 2.9751\n",
      "Epoch [1/1], Step [312900/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [313000/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [313100/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [313200/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [313300/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [313400/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [313500/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [313600/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [313700/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [313800/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [313900/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [314000/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [314100/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [314200/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [314300/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [314400/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [314500/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [314600/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [314700/2550746], Loss: 2.9766\n",
      "Epoch [1/1], Step [314800/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [314900/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [315000/2550746], Loss: 2.9686\n",
      "Epoch [1/1], Step [315100/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [315200/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [315300/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [315400/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [315500/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [315600/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [315700/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [315800/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [315900/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [316000/2550746], Loss: 2.9749\n",
      "Epoch [1/1], Step [316100/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [316200/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [316300/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [316400/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [316500/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [316600/2550746], Loss: 2.9701\n",
      "Epoch [1/1], Step [316700/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [316800/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [316900/2550746], Loss: 2.9941\n",
      "Epoch [1/1], Step [317000/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [317100/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [317200/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [317300/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [317400/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [317500/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [317600/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [317700/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [317800/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [317900/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [318000/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [318100/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [318200/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [318300/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [318400/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [318500/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [318600/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [318700/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [318800/2550746], Loss: 2.9766\n",
      "Epoch [1/1], Step [318900/2550746], Loss: 2.9965\n",
      "Epoch [1/1], Step [319000/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [319100/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [319200/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [319300/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [319400/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [319500/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [319600/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [319700/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [319800/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [319900/2550746], Loss: 2.9731\n",
      "Epoch [1/1], Step [320000/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [320100/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [320200/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [320300/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [320400/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [320500/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [320600/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [320700/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [320800/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [320900/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [321000/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [321100/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [321200/2550746], Loss: 2.9974\n",
      "Epoch [1/1], Step [321300/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [321400/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [321500/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [321600/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [321700/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [321800/2550746], Loss: 2.9966\n",
      "Epoch [1/1], Step [321900/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [322000/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [322100/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [322200/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [322300/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [322400/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [322500/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [322600/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [322700/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [322800/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [322900/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [323000/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [323100/2550746], Loss: 2.9730\n",
      "Epoch [1/1], Step [323200/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [323300/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [323400/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [323500/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [323600/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [323700/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [323800/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [323900/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [324000/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [324100/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [324200/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [324300/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [324400/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [324500/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [324600/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [324700/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [324800/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [324900/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [325000/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [325100/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [325200/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [325300/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [325400/2550746], Loss: 2.9726\n",
      "Epoch [1/1], Step [325500/2550746], Loss: 2.9919\n",
      "Epoch [1/1], Step [325600/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [325700/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [325800/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [325900/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [326000/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [326100/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [326200/2550746], Loss: 2.9753\n",
      "Epoch [1/1], Step [326300/2550746], Loss: 2.9700\n",
      "Epoch [1/1], Step [326400/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [326500/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [326600/2550746], Loss: 2.9938\n",
      "Epoch [1/1], Step [326700/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [326800/2550746], Loss: 2.9724\n",
      "Epoch [1/1], Step [326900/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [327000/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [327100/2550746], Loss: 2.9722\n",
      "Epoch [1/1], Step [327200/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [327300/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [327400/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [327500/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [327600/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [327700/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [327800/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [327900/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [328000/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [328100/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [328200/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [328300/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [328400/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [328500/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [328600/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [328700/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [328800/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [328900/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [329000/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [329100/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [329200/2550746], Loss: 2.9704\n",
      "Epoch [1/1], Step [329300/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [329400/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [329500/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [329600/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [329700/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [329800/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [329900/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [330000/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [330100/2550746], Loss: 2.9951\n",
      "Epoch [1/1], Step [330200/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [330300/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [330400/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [330500/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [330600/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [330700/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [330800/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [330900/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [331000/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [331100/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [331200/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [331300/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [331400/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [331500/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [331600/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [331700/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [331800/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [331900/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [332000/2550746], Loss: 2.9939\n",
      "Epoch [1/1], Step [332100/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [332200/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [332300/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [332400/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [332500/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [332600/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [332700/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [332800/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [332900/2550746], Loss: 2.9717\n",
      "Epoch [1/1], Step [333000/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [333100/2550746], Loss: 2.9756\n",
      "Epoch [1/1], Step [333200/2550746], Loss: 2.9925\n",
      "Epoch [1/1], Step [333300/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [333400/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [333500/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [333600/2550746], Loss: 2.9967\n",
      "Epoch [1/1], Step [333700/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [333800/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [333900/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [334000/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [334100/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [334200/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [334300/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [334400/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [334500/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [334600/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [334700/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [334800/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [334900/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [335000/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [335100/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [335200/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [335300/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [335400/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [335500/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [335600/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [335700/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [335800/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [335900/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [336000/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [336100/2550746], Loss: 2.9947\n",
      "Epoch [1/1], Step [336200/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [336300/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [336400/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [336500/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [336600/2550746], Loss: 2.9746\n",
      "Epoch [1/1], Step [336700/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [336800/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [336900/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [337000/2550746], Loss: 2.9717\n",
      "Epoch [1/1], Step [337100/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [337200/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [337300/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [337400/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [337500/2550746], Loss: 2.9955\n",
      "Epoch [1/1], Step [337600/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [337700/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [337800/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [337900/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [338000/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [338100/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [338200/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [338300/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [338400/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [338500/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [338600/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [338700/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [338800/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [338900/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [339000/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [339100/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [339200/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [339300/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [339400/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [339500/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [339600/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [339700/2550746], Loss: 2.9741\n",
      "Epoch [1/1], Step [339800/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [339900/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [340000/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [340100/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [340200/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [340300/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [340400/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [340500/2550746], Loss: 2.9748\n",
      "Epoch [1/1], Step [340600/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [340700/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [340800/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [340900/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [341000/2550746], Loss: 2.9733\n",
      "Epoch [1/1], Step [341100/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [341200/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [341300/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [341400/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [341500/2550746], Loss: 2.9747\n",
      "Epoch [1/1], Step [341600/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [341700/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [341800/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [341900/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [342000/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [342100/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [342200/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [342300/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [342400/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [342500/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [342600/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [342700/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [342800/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [342900/2550746], Loss: 2.9754\n",
      "Epoch [1/1], Step [343000/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [343100/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [343200/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [343300/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [343400/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [343500/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [343600/2550746], Loss: 2.9748\n",
      "Epoch [1/1], Step [343700/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [343800/2550746], Loss: 2.9949\n",
      "Epoch [1/1], Step [343900/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [344000/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [344100/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [344200/2550746], Loss: 2.9738\n",
      "Epoch [1/1], Step [344300/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [344400/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [344500/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [344600/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [344700/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [344800/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [344900/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [345000/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [345100/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [345200/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [345300/2550746], Loss: 2.9909\n",
      "Epoch [1/1], Step [345400/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [345500/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [345600/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [345700/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [345800/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [345900/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [346000/2550746], Loss: 2.9703\n",
      "Epoch [1/1], Step [346100/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [346200/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [346300/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [346400/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [346500/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [346600/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [346700/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [346800/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [346900/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [347000/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [347100/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [347200/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [347300/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [347400/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [347500/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [347600/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [347700/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [347800/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [347900/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [348000/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [348100/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [348200/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [348300/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [348400/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [348500/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [348600/2550746], Loss: 2.9755\n",
      "Epoch [1/1], Step [348700/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [348800/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [348900/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [349000/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [349100/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [349200/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [349300/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [349400/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [349500/2550746], Loss: 2.9937\n",
      "Epoch [1/1], Step [349600/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [349700/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [349800/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [349900/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [350000/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [350100/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [350200/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [350300/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [350400/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [350500/2550746], Loss: 2.9685\n",
      "Epoch [1/1], Step [350600/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [350700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [350800/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [350900/2550746], Loss: 2.9721\n",
      "Epoch [1/1], Step [351000/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [351100/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [351200/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [351300/2550746], Loss: 2.9916\n",
      "Epoch [1/1], Step [351400/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [351500/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [351600/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [351700/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [351800/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [351900/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [352000/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [352100/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [352200/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [352300/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [352400/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [352500/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [352600/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [352700/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [352800/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [352900/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [353000/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [353100/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [353200/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [353300/2550746], Loss: 2.9711\n",
      "Epoch [1/1], Step [353400/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [353500/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [353600/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [353700/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [353800/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [353900/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [354000/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [354100/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [354200/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [354300/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [354400/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [354500/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [354600/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [354700/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [354800/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [354900/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [355000/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [355100/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [355200/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [355300/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [355400/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [355500/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [355600/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [355700/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [355800/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [355900/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [356000/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [356100/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [356200/2550746], Loss: 2.9937\n",
      "Epoch [1/1], Step [356300/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [356400/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [356500/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [356600/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [356700/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [356800/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [356900/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [357000/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [357100/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [357200/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [357300/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [357400/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [357500/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [357600/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [357700/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [357800/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [357900/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [358000/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [358100/2550746], Loss: 2.9766\n",
      "Epoch [1/1], Step [358200/2550746], Loss: 2.9912\n",
      "Epoch [1/1], Step [358300/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [358400/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [358500/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [358600/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [358700/2550746], Loss: 2.9929\n",
      "Epoch [1/1], Step [358800/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [358900/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [359000/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [359100/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [359200/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [359300/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [359400/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [359500/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [359600/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [359700/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [359800/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [359900/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [360000/2550746], Loss: 2.9730\n",
      "Epoch [1/1], Step [360100/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [360200/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [360300/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [360400/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [360500/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [360600/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [360700/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [360800/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [360900/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [361000/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [361100/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [361200/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [361300/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [361400/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [361500/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [361600/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [361700/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [361800/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [361900/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [362000/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [362100/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [362200/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [362300/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [362400/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [362500/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [362600/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [362700/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [362800/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [362900/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [363000/2550746], Loss: 2.9886\n",
      "Epoch [1/1], Step [363100/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [363200/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [363300/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [363400/2550746], Loss: 2.9946\n",
      "Epoch [1/1], Step [363500/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [363600/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [363700/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [363800/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [363900/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [364000/2550746], Loss: 2.9928\n",
      "Epoch [1/1], Step [364100/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [364200/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [364300/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [364400/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [364500/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [364600/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [364700/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [364800/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [364900/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [365000/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [365100/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [365200/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [365300/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [365400/2550746], Loss: 2.9744\n",
      "Epoch [1/1], Step [365500/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [365600/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [365700/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [365800/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [365900/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [366000/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [366100/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [366200/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [366300/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [366400/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [366500/2550746], Loss: 2.9905\n",
      "Epoch [1/1], Step [366600/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [366700/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [366800/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [366900/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [367000/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [367100/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [367200/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [367300/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [367400/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [367500/2550746], Loss: 2.9746\n",
      "Epoch [1/1], Step [367600/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [367700/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [367800/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [367900/2550746], Loss: 2.9929\n",
      "Epoch [1/1], Step [368000/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [368100/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [368200/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [368300/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [368400/2550746], Loss: 2.9745\n",
      "Epoch [1/1], Step [368500/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [368600/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [368700/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [368800/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [368900/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [369000/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [369100/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [369200/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [369300/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [369400/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [369500/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [369600/2550746], Loss: 2.9738\n",
      "Epoch [1/1], Step [369700/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [369800/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [369900/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [370000/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [370100/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [370200/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [370300/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [370400/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [370500/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [370600/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [370700/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [370800/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [370900/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [371000/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [371100/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [371200/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [371300/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [371400/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [371500/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [371600/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [371700/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [371800/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [371900/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [372000/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [372100/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [372200/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [372300/2550746], Loss: 2.9929\n",
      "Epoch [1/1], Step [372400/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [372500/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [372600/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [372700/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [372800/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [372900/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [373000/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [373100/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [373200/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [373300/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [373400/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [373500/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [373600/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [373700/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [373800/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [373900/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [374000/2550746], Loss: 2.9956\n",
      "Epoch [1/1], Step [374100/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [374200/2550746], Loss: 2.9961\n",
      "Epoch [1/1], Step [374300/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [374400/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [374500/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [374600/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [374700/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [374800/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [374900/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [375000/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [375100/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [375200/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [375300/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [375400/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [375500/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [375600/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [375700/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [375800/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [375900/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [376000/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [376100/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [376200/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [376300/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [376400/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [376500/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [376600/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [376700/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [376800/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [376900/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [377000/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [377100/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [377200/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [377300/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [377400/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [377500/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [377600/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [377700/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [377800/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [377900/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [378000/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [378100/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [378200/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [378300/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [378400/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [378500/2550746], Loss: 2.9937\n",
      "Epoch [1/1], Step [378600/2550746], Loss: 2.9753\n",
      "Epoch [1/1], Step [378700/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [378800/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [378900/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [379000/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [379100/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [379200/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [379300/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [379400/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [379500/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [379600/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [379700/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [379800/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [379900/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [380000/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [380100/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [380200/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [380300/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [380400/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [380500/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [380600/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [380700/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [380800/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [380900/2550746], Loss: 2.9756\n",
      "Epoch [1/1], Step [381000/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [381100/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [381200/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [381300/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [381400/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [381500/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [381600/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [381700/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [381800/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [381900/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [382000/2550746], Loss: 2.9745\n",
      "Epoch [1/1], Step [382100/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [382200/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [382300/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [382400/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [382500/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [382600/2550746], Loss: 2.9963\n",
      "Epoch [1/1], Step [382700/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [382800/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [382900/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [383000/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [383100/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [383200/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [383300/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [383400/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [383500/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [383600/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [383700/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [383800/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [383900/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [384000/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [384100/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [384200/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [384300/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [384400/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [384500/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [384600/2550746], Loss: 2.9924\n",
      "Epoch [1/1], Step [384700/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [384800/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [384900/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [385000/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [385100/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [385200/2550746], Loss: 2.9719\n",
      "Epoch [1/1], Step [385300/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [385400/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [385500/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [385600/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [385700/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [385800/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [385900/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [386000/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [386100/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [386200/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [386300/2550746], Loss: 2.9946\n",
      "Epoch [1/1], Step [386400/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [386500/2550746], Loss: 2.9950\n",
      "Epoch [1/1], Step [386600/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [386700/2550746], Loss: 2.9766\n",
      "Epoch [1/1], Step [386800/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [386900/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [387000/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [387100/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [387200/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [387300/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [387400/2550746], Loss: 2.9751\n",
      "Epoch [1/1], Step [387500/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [387600/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [387700/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [387800/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [387900/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [388000/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [388100/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [388200/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [388300/2550746], Loss: 2.9730\n",
      "Epoch [1/1], Step [388400/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [388500/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [388600/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [388700/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [388800/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [388900/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [389000/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [389100/2550746], Loss: 2.9945\n",
      "Epoch [1/1], Step [389200/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [389300/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [389400/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [389500/2550746], Loss: 2.9733\n",
      "Epoch [1/1], Step [389600/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [389700/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [389800/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [389900/2550746], Loss: 2.9919\n",
      "Epoch [1/1], Step [390000/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [390100/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [390200/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [390300/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [390400/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [390500/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [390600/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [390700/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [390800/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [390900/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [391000/2550746], Loss: 2.9919\n",
      "Epoch [1/1], Step [391100/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [391200/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [391300/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [391400/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [391500/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [391600/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [391700/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [391800/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [391900/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [392000/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [392100/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [392200/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [392300/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [392400/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [392500/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [392600/2550746], Loss: 2.9739\n",
      "Epoch [1/1], Step [392700/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [392800/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [392900/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [393000/2550746], Loss: 2.9731\n",
      "Epoch [1/1], Step [393100/2550746], Loss: 2.9738\n",
      "Epoch [1/1], Step [393200/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [393300/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [393400/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [393500/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [393600/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [393700/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [393800/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [393900/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [394000/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [394100/2550746], Loss: 2.9751\n",
      "Epoch [1/1], Step [394200/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [394300/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [394400/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [394500/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [394600/2550746], Loss: 2.9884\n",
      "Epoch [1/1], Step [394700/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [394800/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [394900/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [395000/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [395100/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [395200/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [395300/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [395400/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [395500/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [395600/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [395700/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [395800/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [395900/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [396000/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [396100/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [396200/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [396300/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [396400/2550746], Loss: 2.9727\n",
      "Epoch [1/1], Step [396500/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [396600/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [396700/2550746], Loss: 2.9928\n",
      "Epoch [1/1], Step [396800/2550746], Loss: 2.9755\n",
      "Epoch [1/1], Step [396900/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [397000/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [397100/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [397200/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [397300/2550746], Loss: 2.9725\n",
      "Epoch [1/1], Step [397400/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [397500/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [397600/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [397700/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [397800/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [397900/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [398000/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [398100/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [398200/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [398300/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [398400/2550746], Loss: 2.9899\n",
      "Epoch [1/1], Step [398500/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [398600/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [398700/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [398800/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [398900/2550746], Loss: 2.9725\n",
      "Epoch [1/1], Step [399000/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [399100/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [399200/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [399300/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [399400/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [399500/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [399600/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [399700/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [399800/2550746], Loss: 2.9735\n",
      "Epoch [1/1], Step [399900/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [400000/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [400100/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [400200/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [400300/2550746], Loss: 2.9749\n",
      "Epoch [1/1], Step [400400/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [400500/2550746], Loss: 2.9931\n",
      "Epoch [1/1], Step [400600/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [400700/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [400800/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [400900/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [401000/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [401100/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [401200/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [401300/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [401400/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [401500/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [401600/2550746], Loss: 2.9918\n",
      "Epoch [1/1], Step [401700/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [401800/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [401900/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [402000/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [402100/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [402200/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [402300/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [402400/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [402500/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [402600/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [402700/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [402800/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [402900/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [403000/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [403100/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [403200/2550746], Loss: 2.9904\n",
      "Epoch [1/1], Step [403300/2550746], Loss: 2.9741\n",
      "Epoch [1/1], Step [403400/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [403500/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [403600/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [403700/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [403800/2550746], Loss: 2.9749\n",
      "Epoch [1/1], Step [403900/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [404000/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [404100/2550746], Loss: 2.9777\n",
      "Epoch [1/1], Step [404200/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [404300/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [404400/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [404500/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [404600/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [404700/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [404800/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [404900/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [405000/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [405100/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [405200/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [405300/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [405400/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [405500/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [405600/2550746], Loss: 2.9713\n",
      "Epoch [1/1], Step [405700/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [405800/2550746], Loss: 2.9745\n",
      "Epoch [1/1], Step [405900/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [406000/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [406100/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [406200/2550746], Loss: 2.9753\n",
      "Epoch [1/1], Step [406300/2550746], Loss: 2.9925\n",
      "Epoch [1/1], Step [406400/2550746], Loss: 2.9950\n",
      "Epoch [1/1], Step [406500/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [406600/2550746], Loss: 2.9942\n",
      "Epoch [1/1], Step [406700/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [406800/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [406900/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [407000/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [407100/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [407200/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [407300/2550746], Loss: 2.9930\n",
      "Epoch [1/1], Step [407400/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [407500/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [407600/2550746], Loss: 2.9745\n",
      "Epoch [1/1], Step [407700/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [407800/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [407900/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [408000/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [408100/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [408200/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [408300/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [408400/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [408500/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [408600/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [408700/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [408800/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [408900/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [409000/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [409100/2550746], Loss: 2.9942\n",
      "Epoch [1/1], Step [409200/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [409300/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [409400/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [409500/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [409600/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [409700/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [409800/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [409900/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [410000/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [410100/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [410200/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [410300/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [410400/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [410500/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [410600/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [410700/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [410800/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [410900/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [411000/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [411100/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [411200/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [411300/2550746], Loss: 2.9752\n",
      "Epoch [1/1], Step [411400/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [411500/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [411600/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [411700/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [411800/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [411900/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [412000/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [412100/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [412200/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [412300/2550746], Loss: 2.9954\n",
      "Epoch [1/1], Step [412400/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [412500/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [412600/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [412700/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [412800/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [412900/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [413000/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [413100/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [413200/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [413300/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [413400/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [413500/2550746], Loss: 2.9970\n",
      "Epoch [1/1], Step [413600/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [413700/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [413800/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [413900/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [414000/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [414100/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [414200/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [414300/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [414400/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [414500/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [414600/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [414700/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [414800/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [414900/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [415000/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [415100/2550746], Loss: 2.9718\n",
      "Epoch [1/1], Step [415200/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [415300/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [415400/2550746], Loss: 2.9702\n",
      "Epoch [1/1], Step [415500/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [415600/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [415700/2550746], Loss: 2.9780\n",
      "Epoch [1/1], Step [415800/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [415900/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [416000/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [416100/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [416200/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [416300/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [416400/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [416500/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [416600/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [416700/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [416800/2550746], Loss: 2.9909\n",
      "Epoch [1/1], Step [416900/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [417000/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [417100/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [417200/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [417300/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [417400/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [417500/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [417600/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [417700/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [417800/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [417900/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [418000/2550746], Loss: 2.9747\n",
      "Epoch [1/1], Step [418100/2550746], Loss: 2.9773\n",
      "Epoch [1/1], Step [418200/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [418300/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [418400/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [418500/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [418600/2550746], Loss: 2.9934\n",
      "Epoch [1/1], Step [418700/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [418800/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [418900/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [419000/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [419100/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [419200/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [419300/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [419400/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [419500/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [419600/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [419700/2550746], Loss: 2.9763\n",
      "Epoch [1/1], Step [419800/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [419900/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [420000/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [420100/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [420200/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [420300/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [420400/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [420500/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [420600/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [420700/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [420800/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [420900/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [421000/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [421100/2550746], Loss: 2.9922\n",
      "Epoch [1/1], Step [421200/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [421300/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [421400/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [421500/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [421600/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [421700/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [421800/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [421900/2550746], Loss: 2.9939\n",
      "Epoch [1/1], Step [422000/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [422100/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [422200/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [422300/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [422400/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [422500/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [422600/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [422700/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [422800/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [422900/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [423000/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [423100/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [423200/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [423300/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [423400/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [423500/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [423600/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [423700/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [423800/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [423900/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [424000/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [424100/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [424200/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [424300/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [424400/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [424500/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [424600/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [424700/2550746], Loss: 2.9947\n",
      "Epoch [1/1], Step [424800/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [424900/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [425000/2550746], Loss: 2.9794\n",
      "Epoch [1/1], Step [425100/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [425200/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [425300/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [425400/2550746], Loss: 2.9758\n",
      "Epoch [1/1], Step [425500/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [425600/2550746], Loss: 2.9848\n",
      "Epoch [1/1], Step [425700/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [425800/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [425900/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [426000/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [426100/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [426200/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [426300/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [426400/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [426500/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [426600/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [426700/2550746], Loss: 2.9750\n",
      "Epoch [1/1], Step [426800/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [426900/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [427000/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [427100/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [427200/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [427300/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [427400/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [427500/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [427600/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [427700/2550746], Loss: 2.9870\n",
      "Epoch [1/1], Step [427800/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [427900/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [428000/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [428100/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [428200/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [428300/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [428400/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [428500/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [428600/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [428700/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [428800/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [428900/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [429000/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [429100/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [429200/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [429300/2550746], Loss: 2.9889\n",
      "Epoch [1/1], Step [429400/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [429500/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [429600/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [429700/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [429800/2550746], Loss: 2.9921\n",
      "Epoch [1/1], Step [429900/2550746], Loss: 2.9745\n",
      "Epoch [1/1], Step [430000/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [430100/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [430200/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [430300/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [430400/2550746], Loss: 2.9807\n",
      "Epoch [1/1], Step [430500/2550746], Loss: 2.9784\n",
      "Epoch [1/1], Step [430600/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [430700/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [430800/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [430900/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [431000/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [431100/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [431200/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [431300/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [431400/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [431500/2550746], Loss: 2.9908\n",
      "Epoch [1/1], Step [431600/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [431700/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [431800/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [431900/2550746], Loss: 2.9864\n",
      "Epoch [1/1], Step [432000/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [432100/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [432200/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [432300/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [432400/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [432500/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [432600/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [432700/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [432800/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [432900/2550746], Loss: 2.9771\n",
      "Epoch [1/1], Step [433000/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [433100/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [433200/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [433300/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [433400/2550746], Loss: 2.9816\n",
      "Epoch [1/1], Step [433500/2550746], Loss: 2.9737\n",
      "Epoch [1/1], Step [433600/2550746], Loss: 2.9902\n",
      "Epoch [1/1], Step [433700/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [433800/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [433900/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [434000/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [434100/2550746], Loss: 2.9740\n",
      "Epoch [1/1], Step [434200/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [434300/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [434400/2550746], Loss: 2.9734\n",
      "Epoch [1/1], Step [434500/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [434600/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [434700/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [434800/2550746], Loss: 2.9756\n",
      "Epoch [1/1], Step [434900/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [435000/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [435100/2550746], Loss: 2.9857\n",
      "Epoch [1/1], Step [435200/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [435300/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [435400/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [435500/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [435600/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [435700/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [435800/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [435900/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [436000/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [436100/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [436200/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [436300/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [436400/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [436500/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [436600/2550746], Loss: 2.9896\n",
      "Epoch [1/1], Step [436700/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [436800/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [436900/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [437000/2550746], Loss: 2.9907\n",
      "Epoch [1/1], Step [437100/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [437200/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [437300/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [437400/2550746], Loss: 2.9893\n",
      "Epoch [1/1], Step [437500/2550746], Loss: 2.9925\n",
      "Epoch [1/1], Step [437600/2550746], Loss: 2.9923\n",
      "Epoch [1/1], Step [437700/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [437800/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [437900/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [438000/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [438100/2550746], Loss: 2.9749\n",
      "Epoch [1/1], Step [438200/2550746], Loss: 2.9748\n",
      "Epoch [1/1], Step [438300/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [438400/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [438500/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [438600/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [438700/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [438800/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [438900/2550746], Loss: 2.9932\n",
      "Epoch [1/1], Step [439000/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [439100/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [439200/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [439300/2550746], Loss: 2.9743\n",
      "Epoch [1/1], Step [439400/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [439500/2550746], Loss: 2.9755\n",
      "Epoch [1/1], Step [439600/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [439700/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [439800/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [439900/2550746], Loss: 2.9943\n",
      "Epoch [1/1], Step [440000/2550746], Loss: 2.9704\n",
      "Epoch [1/1], Step [440100/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [440200/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [440300/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [440400/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [440500/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [440600/2550746], Loss: 2.9946\n",
      "Epoch [1/1], Step [440700/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [440800/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [440900/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [441000/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [441100/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [441200/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [441300/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [441400/2550746], Loss: 2.9802\n",
      "Epoch [1/1], Step [441500/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [441600/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [441700/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [441800/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [441900/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [442000/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [442100/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [442200/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [442300/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [442400/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [442500/2550746], Loss: 2.9942\n",
      "Epoch [1/1], Step [442600/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [442700/2550746], Loss: 2.9774\n",
      "Epoch [1/1], Step [442800/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [442900/2550746], Loss: 2.9957\n",
      "Epoch [1/1], Step [443000/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [443100/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [443200/2550746], Loss: 2.9850\n",
      "Epoch [1/1], Step [443300/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [443400/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [443500/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [443600/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [443700/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [443800/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [443900/2550746], Loss: 2.9919\n",
      "Epoch [1/1], Step [444000/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [444100/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [444200/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [444300/2550746], Loss: 2.9801\n",
      "Epoch [1/1], Step [444400/2550746], Loss: 2.9736\n",
      "Epoch [1/1], Step [444500/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [444600/2550746], Loss: 2.9915\n",
      "Epoch [1/1], Step [444700/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [444800/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [444900/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [445000/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [445100/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [445200/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [445300/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [445400/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [445500/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [445600/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [445700/2550746], Loss: 2.9838\n",
      "Epoch [1/1], Step [445800/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [445900/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [446000/2550746], Loss: 2.9903\n",
      "Epoch [1/1], Step [446100/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [446200/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [446300/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [446400/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [446500/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [446600/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [446700/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [446800/2550746], Loss: 2.9906\n",
      "Epoch [1/1], Step [446900/2550746], Loss: 2.9885\n",
      "Epoch [1/1], Step [447000/2550746], Loss: 2.9868\n",
      "Epoch [1/1], Step [447100/2550746], Loss: 2.9858\n",
      "Epoch [1/1], Step [447200/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [447300/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [447400/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [447500/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [447600/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [447700/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [447800/2550746], Loss: 2.9939\n",
      "Epoch [1/1], Step [447900/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [448000/2550746], Loss: 2.9847\n",
      "Epoch [1/1], Step [448100/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [448200/2550746], Loss: 2.9823\n",
      "Epoch [1/1], Step [448300/2550746], Loss: 2.9833\n",
      "Epoch [1/1], Step [448400/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [448500/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [448600/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [448700/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [448800/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [448900/2550746], Loss: 2.9760\n",
      "Epoch [1/1], Step [449000/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [449100/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [449200/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [449300/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [449400/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [449500/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [449600/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [449700/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [449800/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [449900/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [450000/2550746], Loss: 2.9837\n",
      "Epoch [1/1], Step [450100/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [450200/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [450300/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [450400/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [450500/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [450600/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [450700/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [450800/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [450900/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [451000/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [451100/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [451200/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [451300/2550746], Loss: 2.9779\n",
      "Epoch [1/1], Step [451400/2550746], Loss: 2.9931\n",
      "Epoch [1/1], Step [451500/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [451600/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [451700/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [451800/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [451900/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [452000/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [452100/2550746], Loss: 2.9920\n",
      "Epoch [1/1], Step [452200/2550746], Loss: 2.9825\n",
      "Epoch [1/1], Step [452300/2550746], Loss: 2.9839\n",
      "Epoch [1/1], Step [452400/2550746], Loss: 2.9917\n",
      "Epoch [1/1], Step [452500/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [452600/2550746], Loss: 2.9880\n",
      "Epoch [1/1], Step [452700/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [452800/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [452900/2550746], Loss: 2.9776\n",
      "Epoch [1/1], Step [453000/2550746], Loss: 2.9892\n",
      "Epoch [1/1], Step [453100/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [453200/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [453300/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [453400/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [453500/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [453600/2550746], Loss: 2.9804\n",
      "Epoch [1/1], Step [453700/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [453800/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [453900/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [454000/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [454100/2550746], Loss: 2.9876\n",
      "Epoch [1/1], Step [454200/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [454300/2550746], Loss: 2.9783\n",
      "Epoch [1/1], Step [454400/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [454500/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [454600/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [454700/2550746], Loss: 2.9770\n",
      "Epoch [1/1], Step [454800/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [454900/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [455000/2550746], Loss: 2.9891\n",
      "Epoch [1/1], Step [455100/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [455200/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [455300/2550746], Loss: 2.9766\n",
      "Epoch [1/1], Step [455400/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [455500/2550746], Loss: 2.9785\n",
      "Epoch [1/1], Step [455600/2550746], Loss: 2.9748\n",
      "Epoch [1/1], Step [455700/2550746], Loss: 2.9938\n",
      "Epoch [1/1], Step [455800/2550746], Loss: 2.9887\n",
      "Epoch [1/1], Step [455900/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [456000/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [456100/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [456200/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [456300/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [456400/2550746], Loss: 2.9898\n",
      "Epoch [1/1], Step [456500/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [456600/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [456700/2550746], Loss: 2.9781\n",
      "Epoch [1/1], Step [456800/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [456900/2550746], Loss: 2.9865\n",
      "Epoch [1/1], Step [457000/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [457100/2550746], Loss: 2.9882\n",
      "Epoch [1/1], Step [457200/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [457300/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [457400/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [457500/2550746], Loss: 2.9834\n",
      "Epoch [1/1], Step [457600/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [457700/2550746], Loss: 2.9900\n",
      "Epoch [1/1], Step [457800/2550746], Loss: 2.9829\n",
      "Epoch [1/1], Step [457900/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [458000/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [458100/2550746], Loss: 2.9727\n",
      "Epoch [1/1], Step [458200/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [458300/2550746], Loss: 2.9761\n",
      "Epoch [1/1], Step [458400/2550746], Loss: 2.9754\n",
      "Epoch [1/1], Step [458500/2550746], Loss: 2.9828\n",
      "Epoch [1/1], Step [458600/2550746], Loss: 2.9786\n",
      "Epoch [1/1], Step [458700/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [458800/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [458900/2550746], Loss: 2.9795\n",
      "Epoch [1/1], Step [459000/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [459100/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [459200/2550746], Loss: 2.9859\n",
      "Epoch [1/1], Step [459300/2550746], Loss: 2.9844\n",
      "Epoch [1/1], Step [459400/2550746], Loss: 2.9881\n",
      "Epoch [1/1], Step [459500/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [459600/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [459700/2550746], Loss: 2.9803\n",
      "Epoch [1/1], Step [459800/2550746], Loss: 2.9869\n",
      "Epoch [1/1], Step [459900/2550746], Loss: 2.9911\n",
      "Epoch [1/1], Step [460000/2550746], Loss: 2.9778\n",
      "Epoch [1/1], Step [460100/2550746], Loss: 2.9730\n",
      "Epoch [1/1], Step [460200/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [460300/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [460400/2550746], Loss: 2.9852\n",
      "Epoch [1/1], Step [460500/2550746], Loss: 2.9890\n",
      "Epoch [1/1], Step [460600/2550746], Loss: 2.9762\n",
      "Epoch [1/1], Step [460700/2550746], Loss: 2.9878\n",
      "Epoch [1/1], Step [460800/2550746], Loss: 2.9788\n",
      "Epoch [1/1], Step [460900/2550746], Loss: 2.9769\n",
      "Epoch [1/1], Step [461000/2550746], Loss: 2.9822\n",
      "Epoch [1/1], Step [461100/2550746], Loss: 2.9809\n",
      "Epoch [1/1], Step [461200/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [461300/2550746], Loss: 2.9820\n",
      "Epoch [1/1], Step [461400/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [461500/2550746], Loss: 2.9928\n",
      "Epoch [1/1], Step [461600/2550746], Loss: 2.9841\n",
      "Epoch [1/1], Step [461700/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [461800/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [461900/2550746], Loss: 2.9842\n",
      "Epoch [1/1], Step [462000/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [462100/2550746], Loss: 2.9806\n",
      "Epoch [1/1], Step [462200/2550746], Loss: 2.9757\n",
      "Epoch [1/1], Step [462300/2550746], Loss: 2.9830\n",
      "Epoch [1/1], Step [462400/2550746], Loss: 2.9840\n",
      "Epoch [1/1], Step [462500/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [462600/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [462700/2550746], Loss: 2.9824\n",
      "Epoch [1/1], Step [462800/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [462900/2550746], Loss: 2.9759\n",
      "Epoch [1/1], Step [463000/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [463100/2550746], Loss: 2.9958\n",
      "Epoch [1/1], Step [463200/2550746], Loss: 2.9796\n",
      "Epoch [1/1], Step [463300/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [463400/2550746], Loss: 2.9877\n",
      "Epoch [1/1], Step [463500/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [463600/2550746], Loss: 2.9812\n",
      "Epoch [1/1], Step [463700/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [463800/2550746], Loss: 2.9936\n",
      "Epoch [1/1], Step [463900/2550746], Loss: 2.9913\n",
      "Epoch [1/1], Step [464000/2550746], Loss: 2.9845\n",
      "Epoch [1/1], Step [464100/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [464200/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [464300/2550746], Loss: 2.9787\n",
      "Epoch [1/1], Step [464400/2550746], Loss: 2.9815\n",
      "Epoch [1/1], Step [464500/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [464600/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [464700/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [464800/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [464900/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [465000/2550746], Loss: 2.9910\n",
      "Epoch [1/1], Step [465100/2550746], Loss: 2.9764\n",
      "Epoch [1/1], Step [465200/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [465300/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [465400/2550746], Loss: 2.9935\n",
      "Epoch [1/1], Step [465500/2550746], Loss: 2.9765\n",
      "Epoch [1/1], Step [465600/2550746], Loss: 2.9793\n",
      "Epoch [1/1], Step [465700/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [465800/2550746], Loss: 2.9866\n",
      "Epoch [1/1], Step [465900/2550746], Loss: 2.9789\n",
      "Epoch [1/1], Step [466000/2550746], Loss: 2.9888\n",
      "Epoch [1/1], Step [466100/2550746], Loss: 2.9836\n",
      "Epoch [1/1], Step [466200/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [466300/2550746], Loss: 2.9933\n",
      "Epoch [1/1], Step [466400/2550746], Loss: 2.9862\n",
      "Epoch [1/1], Step [466500/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [466600/2550746], Loss: 2.9798\n",
      "Epoch [1/1], Step [466700/2550746], Loss: 2.9873\n",
      "Epoch [1/1], Step [466800/2550746], Loss: 2.9849\n",
      "Epoch [1/1], Step [466900/2550746], Loss: 2.9827\n",
      "Epoch [1/1], Step [467000/2550746], Loss: 2.9894\n",
      "Epoch [1/1], Step [467100/2550746], Loss: 2.9872\n",
      "Epoch [1/1], Step [467200/2550746], Loss: 2.9863\n",
      "Epoch [1/1], Step [467300/2550746], Loss: 2.9782\n",
      "Epoch [1/1], Step [467400/2550746], Loss: 2.9817\n",
      "Epoch [1/1], Step [467500/2550746], Loss: 2.9875\n",
      "Epoch [1/1], Step [467600/2550746], Loss: 2.9814\n",
      "Epoch [1/1], Step [467700/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [467800/2550746], Loss: 2.9767\n",
      "Epoch [1/1], Step [467900/2550746], Loss: 2.9851\n",
      "Epoch [1/1], Step [468000/2550746], Loss: 2.9860\n",
      "Epoch [1/1], Step [468100/2550746], Loss: 2.9805\n",
      "Epoch [1/1], Step [468200/2550746], Loss: 2.9665\n",
      "Epoch [1/1], Step [468300/2550746], Loss: 2.9831\n",
      "Epoch [1/1], Step [468400/2550746], Loss: 2.9879\n",
      "Epoch [1/1], Step [468500/2550746], Loss: 2.9810\n",
      "Epoch [1/1], Step [468600/2550746], Loss: 2.9867\n",
      "Epoch [1/1], Step [468700/2550746], Loss: 2.9790\n",
      "Epoch [1/1], Step [468800/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [468900/2550746], Loss: 2.9926\n",
      "Epoch [1/1], Step [469000/2550746], Loss: 2.9871\n",
      "Epoch [1/1], Step [469100/2550746], Loss: 2.9901\n",
      "Epoch [1/1], Step [469200/2550746], Loss: 2.9835\n",
      "Epoch [1/1], Step [469300/2550746], Loss: 2.9772\n",
      "Epoch [1/1], Step [469400/2550746], Loss: 2.9874\n",
      "Epoch [1/1], Step [469500/2550746], Loss: 2.9720\n",
      "Epoch [1/1], Step [469600/2550746], Loss: 2.9897\n",
      "Epoch [1/1], Step [469700/2550746], Loss: 2.9895\n",
      "Epoch [1/1], Step [469800/2550746], Loss: 2.9832\n",
      "Epoch [1/1], Step [469900/2550746], Loss: 2.9883\n",
      "Epoch [1/1], Step [470000/2550746], Loss: 2.9821\n",
      "Epoch [1/1], Step [470100/2550746], Loss: 2.9826\n",
      "Epoch [1/1], Step [470200/2550746], Loss: 2.9800\n",
      "Epoch [1/1], Step [470300/2550746], Loss: 2.9811\n",
      "Epoch [1/1], Step [470400/2550746], Loss: 2.9914\n",
      "Epoch [1/1], Step [470500/2550746], Loss: 2.9818\n",
      "Epoch [1/1], Step [470600/2550746], Loss: 2.9950\n",
      "Epoch [1/1], Step [470700/2550746], Loss: 2.9861\n",
      "Epoch [1/1], Step [470800/2550746], Loss: 2.9853\n",
      "Epoch [1/1], Step [470900/2550746], Loss: 2.9808\n",
      "Epoch [1/1], Step [471000/2550746], Loss: 2.9791\n",
      "Epoch [1/1], Step [471100/2550746], Loss: 2.9854\n",
      "Epoch [1/1], Step [471200/2550746], Loss: 2.9792\n",
      "Epoch [1/1], Step [471300/2550746], Loss: 2.9939\n",
      "Epoch [1/1], Step [471400/2550746], Loss: 2.9768\n",
      "Epoch [1/1], Step [471500/2550746], Loss: 2.9813\n",
      "Epoch [1/1], Step [471600/2550746], Loss: 2.9797\n",
      "Epoch [1/1], Step [471700/2550746], Loss: 2.9843\n",
      "Epoch [1/1], Step [471800/2550746], Loss: 2.9846\n",
      "Epoch [1/1], Step [471900/2550746], Loss: 2.9855\n",
      "Epoch [1/1], Step [472000/2550746], Loss: 2.9775\n",
      "Epoch [1/1], Step [472100/2550746], Loss: 2.9819\n",
      "Epoch [1/1], Step [472200/2550746], Loss: 2.9799\n",
      "Epoch [1/1], Step [472300/2550746], Loss: 2.9856\n",
      "Epoch [1/1], Step [472400/2550746], Loss: 2.9956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(transformer\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# モデルの評価\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, dataloader, device):\n",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, epochs, device)\u001b[0m\n\u001b[1;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 勾配の初期化\u001b[39;00m\n\u001b[1;32m     82\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# 逆伝播\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# パラメータ更新\u001b[39;00m\n\u001b[1;32m     85\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     86\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/optim/adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    210\u001b[0m         group,\n\u001b[1;32m    211\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         state_steps,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[0;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/optim/adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/optim/adamw.py:376\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    375\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 376\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    379\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=3e-4)\n",
    "train(transformer, train_loader, optimizer, epochs=1, device=device)\n",
    "\n",
    "# モデルの評価\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()  # モデルを評価モードに設定\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in dataloader:\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "\n",
    "            # モデルの順伝播\n",
    "            logits, loss = model(input_ids, targets=target_ids)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# モデルの評価\n",
    "val_loss = evaluate(transformer, valid_loader, device)\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.0.weight', 'transformer.h.0.mlp.0.bias', 'transformer.h.0.mlp.3.weight', 'transformer.h.0.mlp.3.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.0.weight', 'transformer.h.1.mlp.0.bias', 'transformer.h.1.mlp.3.weight', 'transformer.h.1.mlp.3.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.0.weight', 'transformer.h.2.mlp.0.bias', 'transformer.h.2.mlp.3.weight', 'transformer.h.2.mlp.3.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.0.weight', 'transformer.h.3.mlp.0.bias', 'transformer.h.3.mlp.3.weight', 'transformer.h.3.mlp.3.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.0.weight', 'transformer.h.4.mlp.0.bias', 'transformer.h.4.mlp.3.weight', 'transformer.h.4.mlp.3.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.0.weight', 'transformer.h.5.mlp.0.bias', 'transformer.h.5.mlp.3.weight', 'transformer.h.5.mlp.3.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.0.weight', 'transformer.h.6.mlp.0.bias', 'transformer.h.6.mlp.3.weight', 'transformer.h.6.mlp.3.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.0.weight', 'transformer.h.7.mlp.0.bias', 'transformer.h.7.mlp.3.weight', 'transformer.h.7.mlp.3.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.0.weight', 'transformer.h.8.mlp.0.bias', 'transformer.h.8.mlp.3.weight', 'transformer.h.8.mlp.3.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.0.weight', 'transformer.h.9.mlp.0.bias', 'transformer.h.9.mlp.3.weight', 'transformer.h.9.mlp.3.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.0.weight', 'transformer.h.10.mlp.0.bias', 'transformer.h.10.mlp.3.weight', 'transformer.h.10.mlp.3.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.0.weight', 'transformer.h.11.mlp.0.bias', 'transformer.h.11.mlp.3.weight', 'transformer.h.11.mlp.3.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])\n"
     ]
    }
   ],
   "source": [
    "save_data = {\n",
    "    \"model_state_dict\": transformer.state_dict(),\n",
    "    \"config\": config\n",
    "}\n",
    "\n",
    "torch.save(save_data, \"transformer_with_config.pth\")\n",
    "\n",
    "print(transformer.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoshida/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/tmp/ipykernel_425497/2549076071.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  load_data = torch.load(\"transformer_with_config.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トランスフォーマーの総パラメータ数: 85.62M\n"
     ]
    }
   ],
   "source": [
    "# モデルの読み込み\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from GPT2 import Transformer, ModelConfig\n",
    "from Tokenizer import Tokenizer\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.tokens = self.tokenizer.encode(text, eot=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.tokens[idx:idx+self.block_size]), torch.tensor(self.tokens[idx+1:idx+self.block_size+1])\n",
    "\n",
    "# 相対パスを指定してテキストファイルを読み込む\n",
    "file_path1 = \"../data/mini_ptb.train.txt\"\n",
    "file_path2 = \"../data/mini_ptb.valid.txt\"\n",
    "file_path3 = \"../data/mini_ptb.test.txt\"\n",
    "\n",
    "# ファイルを開いて内容を読み込む\n",
    "with open(file_path1, \"r\", encoding=\"utf-8\") as file:\n",
    "    mini_ptb_train_text = file.read()\n",
    "\n",
    "with open(file_path2, \"r\", encoding=\"utf-8\") as file:\n",
    "    mini_ptb_valid_text = file.read()\n",
    "\n",
    "with open(file_path3, \"r\", encoding=\"utf-8\") as file:\n",
    "    mini_ptb_test_text = file.read()\n",
    "\n",
    "unique_chars_in_train_text = sorted(list(set(mini_ptb_train_text)))\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "load_data = torch.load(\"transformer_with_config.pth\")\n",
    "config = load_data[\"config\"]\n",
    "transformer = Transformer(config)\n",
    "transformer.load_state_dict(load_data[\"model_state_dict\"])\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "tokenizer = Tokenizer(unique_chars_in_train_text)\n",
    "# データセットの作成\n",
    "train_dataset = TextDataset(mini_ptb_train_text, tokenizer, block_size)\n",
    "valid_dataset = TextDataset(mini_ptb_valid_text, tokenizer, block_size)\n",
    "test_dataset = TextDataset(mini_ptb_test_text, tokenizer, block_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to go to honor use \n"
     ]
    }
   ],
   "source": [
    "#生成\n",
    "input_text = \"I want to go to \"\n",
    "input_tokens = tokenizer.encode(input_text)\n",
    "input_tokens = torch.tensor(input_tokens).unsqueeze(0).to(device)\n",
    "\n",
    "output_sentence = transformer.generate(input_tokens, max_new_tokens=10, tokenizer = tokenizer)\n",
    "\n",
    "print(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perplexityの計算\n",
    "import math\n",
    "\n",
    "def calculate_perplexity(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, target_ids in dataloader:\n",
    "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
    "            _, loss = model(input_ids, targets=target_ids)\n",
    "            total_loss += loss.item()* input_ids.size(1)\n",
    "            total_tokens += input_ids.size(1)\n",
    "\n",
    "    perplexity = math.exp(total_loss / total_tokens)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.22\n"
     ]
    }
   ],
   "source": [
    "# 例: Perplexityの計算\n",
    "perplexity = calculate_perplexity(transformer, train_loader, device)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文法的誤り訂正タスク\n",
    "def correct_sentence(model, tokenizer, sentence, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    文法的誤りを含む文を訂正し、正しい文を生成する関数。\n",
    "\n",
    "    Args:\n",
    "        model: 学習済みの言語モデル（Transformerモデル）。\n",
    "        tokenizer: トークナイザー（入力文をトークン化し、出力文をデコード）。\n",
    "        sentence (str): 文法的誤りを含む入力文。\n",
    "        max_new_tokens (int): 生成する最大トークン数。\n",
    "\n",
    "    Returns:\n",
    "        corrected_sentence (str): 訂正された文法的に正しい文。\n",
    "    \"\"\"\n",
    "    model.eval()  # モデルを推論モードに設定\n",
    "    # 入力文をトークン化し、テンソルに変換\n",
    "    token_ids = torch.tensor(tokenizer.encode(sentence, eot=True), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # モデルによるテキスト生成\n",
    "    with torch.no_grad():  # 勾配計算を無効化し、推論を高速化\n",
    "        output_ids = model.generate(token_ids, max_new_tokens=max_new_tokens, tokenizer=tokenizer)\n",
    "\n",
    "    # 生成されたトークンをデコードして文字列に戻す\n",
    "    corrected_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[288, 293, 258, 294]\n",
      "入力文: I have a\n",
      "訂正後の文: ns t\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"I have a\"\n",
    "input_tokens = torch.tensor(tokenizer.encode(input_sentence, eot=True), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "correct_ids =transformer.correct_sentence(input_tokens, max_new_tokens=4)\n",
    "corrected_sentence = tokenizer.decode(correct_ids.squeeze(0).tolist())\n",
    "print(correct_ids.squeeze(0).tolist())\n",
    "print(f\"入力文: {input_sentence}\")\n",
    "print(f\"訂正後の文: {corrected_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 301\n"
     ]
    }
   ],
   "source": [
    "# 生成されたトークンIDのリストを取得\n",
    "generated_ids = tokenizer.encode(corrected_sentence, eot=True)\n",
    "\n",
    "# トークンIDの範囲確認\n",
    "vocab_size = len(tokenizer.idx_to_str)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "for token_id in generated_ids:\n",
    "    if token_id >= vocab_size:\n",
    "        print(f\"Out-of-range token ID detected: {token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    評価データセットを使ってモデルのPerplexityやAccuracyを計算する関数。\n",
    "    Args:\n",
    "        model (nn.Module): 言語モデル\n",
    "        dataloader (DataLoader): 評価用データローダー\n",
    "        device (str): デバイス（CPU or GPU）\n",
    "    Returns:\n",
    "        dict: PerplexityやAccuracyを含む評価結果\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            _, loss = model(inputs, targets)\n",
    "            total_loss += loss.item() * targets.numel()  # トークン数で重み付け\n",
    "            total_tokens += targets.numel()\n",
    "\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\n",
    "    return {\"Perplexity\": perplexity.item()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "# 文字単位トークン\n",
    "sentence = \"cat sat on the mat\"\n",
    "inputs = tokenizer.encode(sentence, return_tensors=\"pt\").unsqueeze(0)\n",
    "\n",
    "# モデルの出力 (Attentionを含む)\n",
    "logits, loss, attentions = transformer.forward(inputs, targets=inputs)\n",
    "\n",
    "def plot_attention(attn, layer_idx=0, head_idx=0):\n",
    "    \"\"\"\n",
    "    特定の層とヘッドのAttention重みをヒートマップで可視化する\n",
    "    \"\"\"\n",
    "    attn_weights = attn[layer_idx][0, head_idx].detach().cpu().numpy()  # Attention重みを取り出す\n",
    "    plt.imshow(attn_weights, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Layer {layer_idx}, Head {head_idx} Attention Map\")\n",
    "    plt.xlabel(\"Keys\")\n",
    "    plt.ylabel(\"Queries\")\n",
    "    plt.show()\n",
    "\n",
    "# 1層目、1ヘッド目のAttentionを可視化\n",
    "plot_attention(attentions, layer_idx=0, head_idx=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_attention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m layer_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m head_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):  \u001b[38;5;66;03m# 8はHead数（適切な値に変更してください）\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mplot_attention\u001b[49m(attentions, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx, head_idx\u001b[38;5;241m=\u001b[39mhead_idx)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_attention' is not defined"
     ]
    }
   ],
   "source": [
    "# 例えばLayer 0のすべてのHeadを表示する\n",
    "layer_idx = 0\n",
    "for head_idx in range(8):  # 8はHead数（適切な値に変更してください）\n",
    "    plot_attention(attentions, layer_idx=layer_idx, head_idx=head_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成文法評価"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
